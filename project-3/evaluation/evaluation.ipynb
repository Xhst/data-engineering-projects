{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione\n",
    "\n",
    "1. Creare tot sistemi diversi (per esempio, lucene, bert, scibert, tabert (con e senza contesto))\n",
    "2. Creare un sottoinsieme dei paper (tipo 20) da usare come ground truth (o a caso oppure con lucene i più rilevanti per argomento che abbiano tabelle interessanti) - cerchiamo di limitare il numero di tabelle a ca. 50\n",
    "3. Per ogni query $q \\in Q$ (min 5):\n",
    "    1. Fare il ranking a mano delle tabelle\n",
    "        - Salviamo i ranking per ogni query in un json, con le informazioni rilevanti, tipo il ranking, il valore di rilevanza per ogni elemento etc.\n",
    "    2. Interrogare ogni sistema sulla query\n",
    "    3. Calcolare le metriche: \n",
    "        - Reciprocal Rank: $\\text{RR}_q = \\frac{1}{rank_i}$ dove $i$ è l’elemento più rilevante.\n",
    "            - nella pratica possiamo controllare se l’elemento scelto dal motore ha almeno lo score massimo (potrebbero esserci dei parimerito)\n",
    "        - Normalized Discounted Cumulative Gain con taglio $\\text{K} = \\set{5,15}$:\n",
    "            \n",
    "            $$\n",
    "            \\text{NDCG@K}_q = \\frac{\\text{DCG@K}_q}{\\text{IDCG@K}_q}\n",
    "            $$\n",
    "            \n",
    "            - dove dividiamo il $\\text{DCG@K}_q = rel_1 + \\sum_{i=2}^K \\frac{rel_i}{\\log_2 (i + 1)}$ con quello ideale, cioè dove il ranking è il migliore possibile\n",
    "4. Calcolare la media delle metriche:\n",
    "    - Mean Reciprocal Rank: $\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{RR}_q$\n",
    "    - Media dei NDCG: $\\frac{1}{|Q|} \\sum_{q \\in Q} \\text{NDCG@K}_q$\n",
    "\n",
    "### Query (in verde stesso ranking ma proviamo sinonimi)\n",
    "\n",
    "1. NDCG su dataset movielens ✅\n",
    "2. Recommender systems Recall su dataset goodbook ✅\n",
    "3. Recommender systems MRR ✅\n",
    "4. Deep Learning dataset Apple Flower ✅\n",
    "5. Deep Learning GPT3 precision f1 ✅\n",
    "6. Deep Learning GPT3 precision f-measure ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "results_file = \"./results_hybrid.json\"\n",
    "ground_truth_path = \"./ground_truth\"\n",
    "num_queries = 6\n",
    "\n",
    "# model -> method -> query -> (position, id) \n",
    "results: dict[str, dict[str, dict[str, dict[str, str]]]] = {}\n",
    "\n",
    "# query -> (position, table[table_id, query_id, rel]) \n",
    "ground_truth: dict[str, dict[str, dict[str, str]]] = {}\n",
    "\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "    \n",
    "for i in range(1, num_queries + 1):\n",
    "    query_id = f\"q{i}\"\n",
    "    with open(ground_truth_path + f\"/{query_id}_rank.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        ground_truth[query_id] = json.load(file)\n",
    "            \n",
    "\n",
    "def compare_id(id1: str, id2: str) -> bool:\n",
    "    id1 = str.lower(re.sub(r'v\\d+', '', id1))\n",
    "    id2 = str.lower(re.sub(r'v\\d+', '', id2))\n",
    "\n",
    "    return id1 == id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value for lucene using method: bm25 is 0.20833333333333334 --- best not found in 4/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_embedding is 0.05000000000000001 --- best not found in 4/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_embedding is 0.05555555555555555 --- best not found in 4/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_ref_embedding is 0.19047619047619047 --- best not found in 4/6 queries.\n",
      "MRR value for bert-base-uncased using method: weighted_embedding is 0.047619047619047616 --- best not found in 4/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_embedding is 0.03333333333333333 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_embedding is 0.05416666666666667 --- best not found in 4/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.125 --- best not found in 4/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: weighted_embedding is 0.037037037037037035 --- best not found in 4/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.016666666666666666 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.03968253968253968 --- best not found in 4/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.011904761904761904 --- best not found in 5/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_cap_embedding is 0.20833333333333334 --- best not found in 4/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_cap_ref_embedding is 0.20833333333333334 --- best not found in 4/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: weighted_embedding is 0.0625 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_cap_embedding is 0.20833333333333334 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_cap_ref_embedding is 0.20833333333333334 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: weighted_embedding is 0.20833333333333334 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_embedding is 0.09722222222222221 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_ref_embedding is 0.06018518518518518 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: weighted_embedding is 0.09722222222222221 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_embedding is 0.06547619047619048 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_ref_embedding is 0.0625 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: weighted_embedding is 0.075 --- best not found in 4/6 queries.\n"
     ]
    }
   ],
   "source": [
    "mrr_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    mrr_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_rr = 0\n",
    "        not_founds = 0\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            best_tables_ids: list[str] = []\n",
    "            best_table =  ground_truth[query_id][\"1\"]\n",
    "            \n",
    "            # this checks for equal relevance tables other than the first position\n",
    "            for pos, table in ground_truth[query_id].items():\n",
    "                if table[\"rel\"] == best_table[\"rel\"]:\n",
    "                    best_tables_ids.append(table[\"paper_id\"] + \"#\" + table[\"table_id\"])\n",
    "            \n",
    "            rr = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                if (table_id in best_tables_ids): rr = 1.0 / float(pos)\n",
    "            \n",
    "            if rr == 0: not_founds += 1\n",
    "            sum_rr += rr\n",
    "            \n",
    "        mrr = sum_rr / num_queries\n",
    "        mrr_values[model][method] = mrr\n",
    "        print(f\"MRR value for {model} using method: {method} is {mrr} --- best not found in {not_founds}/{num_queries} queries.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== lucene --> bm25 ==============\n",
      "Average NDCG@5 is 0.9214215230998138.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.6241123305534374.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.5993533732831239.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.5559778214570784.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.5665435088985525.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.6809919308527355.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.6797787542922435.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.5131625572007309.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.5816040473636793.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.5177521424429389.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.46056144447118824.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.3818974304306249.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.5231405709447273.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.8898029643540261.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.8450019325705456.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.9337763415287165.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.8958585116891989.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.8114090068950669.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.929145052516951.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.906092866266504.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.8642840030473726.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.8874118798157687.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.8415450985092011.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.9096234993857024.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.89637124514452.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_embedding ==============\n",
      "Average NDCG@5 is 0.8886040478253022.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_cap_embedding ==============\n",
      "Average NDCG@5 is 0.8276094431273672.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@5 is 0.8969400091258194.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> weighted_embedding ==============\n",
      "Average NDCG@5 is 0.9055556136418711.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "K = 15\n",
    "idcg_values: dict[str, float] = {}\n",
    "\n",
    "for query_id, ranking in ground_truth.items():    \n",
    "    idcg = 0\n",
    "\n",
    "    for i in range(1, K + 1):\n",
    "        table = ground_truth[query_id][str(i)]\n",
    "        table_id = table[\"paper_id\"] + \"#\" + table[\"table_id\"]\n",
    "        rel: float = float(table[\"rel\"])\n",
    "\n",
    "        idcg += rel / math.log2(i + 1)\n",
    "    \n",
    "    idcg_values[query_id] = idcg\n",
    "\n",
    "for model, methods in results.items():\n",
    "    for method, queries in methods.items():\n",
    "        print(f\"============== {model} --> {method} ==============\")\n",
    "        sum_ndcg: float = 0\n",
    "        for query_id, ranking in queries.items():\n",
    "            dcg = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                rel: float = 0\n",
    "                \n",
    "                for _, gt_table in ground_truth[query_id].items():\n",
    "                   gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                   if compare_id(table_id, gt_table_id):\n",
    "                       rel = float(gt_table[\"rel\"])\n",
    "                \n",
    "                dcg += rel / math.log2(int(pos) + 1)\n",
    "                \n",
    "            ndcg = dcg / idcg_values[query_id] if dcg / idcg_values[query_id] <= 1 else 1\n",
    "            sum_ndcg += ndcg\n",
    "            \n",
    "            #print(f\"NDCG@{K} for query {query_id} is {ndcg}.\")\n",
    "        print(f\"Average NDCG@{K} is {sum_ndcg / num_queries}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Precision@15 of lucene with bm25 for query q1 is: 0.8353537203537204\n",
      "Avg Precision@15 of lucene with bm25 for query q2 is: 0.49310522810522806\n",
      "Avg Precision@15 of lucene with bm25 for query q3 is: 0.64999000999001\n",
      "Avg Precision@15 of lucene with bm25 for query q4 is: 0.5959721759721762\n",
      "Avg Precision@15 of lucene with bm25 for query q5 is: 0.909818144818145\n",
      "Avg Precision@15 of lucene with bm25 for query q6 is: 0.9507688607688608\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q1 is: 0.5697006697006698\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q2 is: 0.20914270914270913\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q3 is: 0.4165806415806415\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q4 is: 0.6762839012839011\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q5 is: 0.5587747437747439\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q6 is: 0.5703315203315202\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q1 is: 0.6892997742997743\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q2 is: 0.3015789765789766\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q3 is: 0.5190657490657491\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q4 is: 0.6720518370518371\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q5 is: 0.26166167166167165\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q6 is: 0.5769476819476819\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q1 is: 0.568955858955859\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q2 is: 0.6535366485366486\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q3 is: 0.2953653753653754\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q4 is: 0.6330604580604581\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q5 is: 0.2636393236393237\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q6 is: 0.32131368631368623\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q1 is: 0.7665756465756465\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q2 is: 0.4223396973396974\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q3 is: 0.16335238835238836\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q4 is: 0.5338541088541089\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q5 is: 0.4029563029563029\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q6 is: 0.4826331076331076\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q1 is: 0.6778206978206978\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q2 is: 0.40486032486032486\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q3 is: 0.18737355237355235\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q4 is: 0.7279248529248529\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q5 is: 0.7487978687978688\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q6 is: 0.5407018907018908\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q1 is: 0.7628719428719428\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q2 is: 0.5435101935101936\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q3 is: 0.25858881858881866\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q4 is: 0.5802810152810153\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q5 is: 0.49631442631442635\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q6 is: 0.592999407999408\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q1 is: 0.7118642468642468\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q2 is: 0.8051735301735302\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q3 is: 0.2231109631109631\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q4 is: 0.6301968401968402\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q5 is: 0.04787545787545787\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q6 is: 0.07424945424945426\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q1 is: 0.82506364006364\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q2 is: 0.593058793058793\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q3 is: 0.09142283642283641\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q4 is: 0.48869537869537866\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q5 is: 0.37578292078292086\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q6 is: 0.4985549635549635\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q1 is: 0.4018940318940319\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q2 is: 0.18956765456765456\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q3 is: 0.13554982054982054\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q4 is: 0.4671850371850372\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q5 is: 0.583029933029933\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q6 is: 0.4840368890368891\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q1 is: 0.5539253339253338\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q2 is: 0.1994146594146594\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q3 is: 0.10343748843748843\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q4 is: 0.2873454323454323\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q5 is: 0.5101768601768603\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q6 is: 0.4998064898064898\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q1 is: 0.5247511747511747\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q2 is: 0.3670039220039219\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q3 is: 0.03354090354090354\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q4 is: 0.14121193621193623\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q5 is: 0.4114235764235764\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q6 is: 0.27253468753468757\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q1 is: 0.63485532985533\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q2 is: 0.3417898767898767\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q3 is: 0.057881932881932885\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q4 is: 0.41967421467421473\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q5 is: 0.6661011211011212\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q6 is: 0.555996965996966\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q1 is: 0.7093700743700744\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q2 is: 0.25083583083583083\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q3 is: 0.77025123025123\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q4 is: 0.6981886631886632\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q5 is: 0.8618857068857069\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q6 is: 0.7841384541384541\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q1 is: 0.9284672734672734\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q2 is: 0.6954380804380804\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q3 is: 0.6503281903281902\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q4 is: 0.5959721759721762\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q5 is: 0.5793055093055094\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q6 is: 0.5249644799644799\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q1 is: 0.7272272172272172\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q2 is: 0.5487971287971287\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q3 is: 0.8033176083176082\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q4 is: 0.7042492692492692\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q5 is: 0.8773924223924223\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q6 is: 0.7683953083953081\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q1 is: 0.8910391460391461\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q2 is: 0.38657083657083646\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q3 is: 0.8036276686276688\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q5 is: 0.6007018907018906\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q6 is: 0.5473990823990824\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q1 is: 0.6694873644873645\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q2 is: 0.3355431605431605\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q3 is: 0.572981092981093\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q4 is: 0.746073001073001\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q5 is: 0.8469972619972619\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q6 is: 0.8884424834424836\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q1 is: 0.83019425019425\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q2 is: 0.7605150405150404\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q3 is: 0.591018426018426\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q4 is: 0.655887075887076\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q5 is: 0.8239203389203388\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q6 is: 0.9084672734672734\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q1 is: 0.6871841121841122\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q2 is: 0.751252451252451\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q3 is: 0.641998186998187\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q4 is: 0.7666814666814668\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q5 is: 0.9261738261738262\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q6 is: 0.9438417138417139\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q1 is: 0.8787392237392236\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q2 is: 0.4716037666037666\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q3 is: 0.391946941946942\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q4 is: 0.6603315203315204\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q5 is: 0.8214905464905462\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q6 is: 0.8447139897139896\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q1 is: 0.7376016576016574\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q2 is: 0.3207035557035557\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q3 is: 0.9264343064343065\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q5 is: 0.8965947015947016\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q6 is: 0.8616252266252266\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q1 is: 0.8318857068857068\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q2 is: 0.3903339253339253\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q3 is: 0.7143534243534243\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q4 is: 0.5959721759721762\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q5 is: 0.7189012839012837\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q6 is: 0.63485532985533\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q1 is: 0.8088979538979537\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q2 is: 0.4110987160987161\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q3 is: 0.9132291782291782\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q4 is: 0.6827429977429978\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q5 is: 0.9071726421726425\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q6 is: 0.8292931142931143\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q1 is: 0.7727587227587228\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q2 is: 0.3888276538276539\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q3 is: 0.7591401191401189\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q4 is: 0.6538541088541089\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q5 is: 0.8171559921559921\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q6 is: 0.7108308358308358\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q1 is: 0.626038406038406\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q2 is: 0.307049432049432\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q3 is: 0.6838145188145188\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q5 is: 0.8451983201983201\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q6 is: 0.8500090650090648\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q1 is: 0.8841327191327191\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q2 is: 0.40189070189070186\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q3 is: 0.669111259111259\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q4 is: 0.6051785251785253\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q5 is: 0.676974136974137\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q6 is: 0.658078773078773\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q1 is: 0.6506952306952306\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q2 is: 0.30987771487771487\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q3 is: 0.55493302993303\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q5 is: 0.8650884300884301\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q6 is: 0.8698991748991748\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q1 is: 0.8583142783142784\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q2 is: 0.37048692048692045\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q3 is: 0.7522064972064971\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q4 is: 0.66496521996522\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q5 is: 0.7920461020461018\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q6 is: 0.7333705183705184\n"
     ]
    }
   ],
   "source": [
    "# Precision at k\n",
    "def precision_at_k(k: int):\n",
    "    precision_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "    for model, methods in results.items():\n",
    "        precision_values[model] = {}\n",
    "        for method, queries in methods.items():\n",
    "            precision_values[model][method] = {}\n",
    "            for query_id, ranking in queries.items():\n",
    "                relevant = 0\n",
    "                for pos, table_id in ranking.items():\n",
    "                    if int(pos) > k: break\n",
    "                    \n",
    "                    rel: float = 0\n",
    "                    \n",
    "                    # find relevance\n",
    "                    for _, gt_table in ground_truth[query_id].items():\n",
    "                        gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                        if compare_id(table_id, gt_table_id):\n",
    "                            rel = float(gt_table[\"rel\"])\n",
    "                    \n",
    "                    if rel > 0: relevant += 1\n",
    "                \n",
    "                precision = relevant / k\n",
    "                precision_values[model][method][query_id] = precision\n",
    "    \n",
    "    return precision_values\n",
    "\n",
    "# Avg Precision at K\n",
    "K = 15\n",
    "# model -> method -> query, avg_precision@k\n",
    "ap_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    ap_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        ap_values[model][method] = {}\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            sum_p = 0\n",
    "            \n",
    "            for k in range(1, K + 1):\n",
    "                precision_values_at_k = precision_at_k(k)\n",
    "                sum_p += precision_values_at_k[model][method][query_id]\n",
    "                \n",
    "            ap_values[model][method][query_id] = sum_p / K\n",
    "            print(f\"Avg Precision@{K} of {model} with {method} for query {query_id} is: {ap_values[model][method][query_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@15 for lucene using method: bm25 is 0.7391680233346901.\n",
      "MAP@15 for bert-base-uncased using method: tab_embedding is 0.5001356976356978.\n",
      "MAP@15 for bert-base-uncased using method: tab_cap_embedding is 0.5034342817676152.\n",
      "MAP@15 for bert-base-uncased using method: tab_cap_ref_embedding is 0.4559785584785585.\n",
      "MAP@15 for bert-base-uncased using method: weighted_embedding is 0.4619518752852086.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_embedding is 0.5479131979131979.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_cap_embedding is 0.5390943007609674.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.4154117487450821.\n",
      "MAP@15 for distilbert-base-uncased using method: weighted_embedding is 0.4787630887630887.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.37687722771056104.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.35901771068437743.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.2917443667443667.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.4460499068832402.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_embedding is 0.6791116599449932.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_cap_embedding is 0.6624126182459517.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_cap_ref_embedding is 0.7382298257298255.\n",
      "MAP@15 for all-mpnet-base-v2 using method: weighted_embedding is 0.6512728629395297.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_embedding is 0.6765873940873942.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_cap_embedding is 0.7616670675004008.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_cap_ref_embedding is 0.7861886261886263.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: weighted_embedding is 0.6781376648043315.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_embedding is 0.7368763335430001.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_embedding is 0.6477169743836411.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_ref_embedding is 0.7587391004057671.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: weighted_embedding is 0.6837612387612387.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_embedding is 0.6650680492347159.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_embedding is 0.6492276858943525.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_ref_embedding is 0.6547986889653556.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: weighted_embedding is 0.695231589398256.\n"
     ]
    }
   ],
   "source": [
    "# MAP@K (K è quello sopra)\n",
    "map_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    map_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_ap = 0\n",
    "        \n",
    "        for avg_prec in ap_values[model][method].values():\n",
    "            sum_ap += avg_prec\n",
    "        \n",
    "        map_value = sum_ap / num_queries\n",
    "        map_values[model][method] = map_value\n",
    "            \n",
    "        print(f\"MAP@{K} for {model} using method: {method} is {map_value}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
