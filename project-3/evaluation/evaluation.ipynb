{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione\n",
    "\n",
    "1. Creare tot sistemi diversi (per esempio, lucene, bert, scibert, tabert (con e senza contesto))\n",
    "2. Creare un sottoinsieme dei paper (tipo 20) da usare come ground truth (o a caso oppure con lucene i più rilevanti per argomento che abbiano tabelle interessanti) - cerchiamo di limitare il numero di tabelle a ca. 50\n",
    "3. Per ogni query $q \\in Q$ (min 5):\n",
    "    1. Fare il ranking a mano delle tabelle\n",
    "        - Salviamo i ranking per ogni query in un json, con le informazioni rilevanti, tipo il ranking, il valore di rilevanza per ogni elemento etc.\n",
    "    2. Interrogare ogni sistema sulla query\n",
    "    3. Calcolare le metriche: \n",
    "        - Reciprocal Rank: $\\text{RR}_q = \\frac{1}{rank_i}$ dove $i$ è l’elemento più rilevante.\n",
    "            - nella pratica possiamo controllare se l’elemento scelto dal motore ha almeno lo score massimo (potrebbero esserci dei parimerito)\n",
    "        - Normalized Discounted Cumulative Gain con taglio $\\text{K} = \\set{5,15}$:\n",
    "            \n",
    "            $$\n",
    "            \\text{NDCG@K}_q = \\frac{\\text{DCG@K}_q}{\\text{IDCG@K}_q}\n",
    "            $$\n",
    "            \n",
    "            - dove dividiamo il $\\text{DCG@K}_q = rel_1 + \\sum_{i=2}^K \\frac{rel_i}{\\log_2 (i + 1)}$ con quello ideale, cioè dove il ranking è il migliore possibile\n",
    "4. Calcolare la media delle metriche:\n",
    "    - Mean Reciprocal Rank: $\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{RR}_q$\n",
    "    - Media dei NDCG: $\\frac{1}{|Q|} \\sum_{q \\in Q} \\text{NDCG@K}_q$\n",
    "\n",
    "### Query (in verde stesso ranking ma proviamo sinonimi)\n",
    "\n",
    "1. NDCG su dataset movielens ✅\n",
    "2. Recommender systems Recall su dataset goodbook ✅\n",
    "3. Recommender systems MRR ✅\n",
    "4. Deep Learning dataset Apple Flower ✅\n",
    "5. Deep Learning GPT3 precision f1 ✅\n",
    "6. Deep Learning GPT3 precision f-measure ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_file = \"./results.json\"\n",
    "ground_truth_path = \"./ground_truth\"\n",
    "num_queries = 6\n",
    "\n",
    "# model -> method -> query -> (position, id) \n",
    "results: dict[str, dict[str, dict[str, dict[str, str]]]] = {}\n",
    "\n",
    "# query -> (position, id) \n",
    "ground_truth: dict[str, dict[str, str]] = {}\n",
    "\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "    \n",
    "for i in range(1, num_queries + 1):\n",
    "    query_id = f\"q{i}\"\n",
    "    with open(ground_truth_path + f\"/{query_id}_rank.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        ground_truth[query_id] = json.load(file)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value for lucene using method: bm25 is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.015151515151515152 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.0 --- best not found in 6/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.023809523809523808 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_embedding is 0.020833333333333332 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_embedding is 0.013888888888888888 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_ref_embedding is 0.0 --- best not found in 6/6 queries.\n",
      "MRR value for bert-base-uncased using method: weighted_embedding is 0.013888888888888888 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_embedding is 0.023809523809523808 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_embedding is 0.027777777777777776 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.0 --- best not found in 6/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: weighted_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n"
     ]
    }
   ],
   "source": [
    "mrr_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    mrr_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_rr = 0\n",
    "        not_founds = 0\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            best_table: str = ground_truth[query_id][\"1\"]\n",
    "            best_table_id = best_table[\"paper_id\"] + \"#\" + best_table[\"table_id\"]\n",
    "            \n",
    "            rr = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                if (table_id == best_table_id): rr = 1.0 / float(pos)\n",
    "            \n",
    "            if rr == 0: not_founds += 1\n",
    "            sum_rr += rr\n",
    "            \n",
    "            \n",
    "        \n",
    "        mrr = sum_rr / num_queries\n",
    "        mrr_values[model][method] = mrr\n",
    "        print(f\"MRR value for {model} using method: {method} is {mrr} --- best not found in {not_founds}/{num_queries} queries.\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
