{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione\n",
    "\n",
    "1. Creare tot sistemi diversi (per esempio, lucene, bert, scibert, tabert (con e senza contesto))\n",
    "2. Creare un sottoinsieme dei paper (tipo 20) da usare come ground truth (o a caso oppure con lucene i più rilevanti per argomento che abbiano tabelle interessanti) - cerchiamo di limitare il numero di tabelle a ca. 50\n",
    "3. Per ogni query $q \\in Q$ (min 5):\n",
    "    1. Fare il ranking a mano delle tabelle\n",
    "        - Salviamo i ranking per ogni query in un json, con le informazioni rilevanti, tipo il ranking, il valore di rilevanza per ogni elemento etc.\n",
    "    2. Interrogare ogni sistema sulla query\n",
    "    3. Calcolare le metriche: \n",
    "        - Reciprocal Rank: $\\text{RR}_q = \\frac{1}{rank_i}$ dove $i$ è l’elemento più rilevante.\n",
    "            - nella pratica possiamo controllare se l’elemento scelto dal motore ha almeno lo score massimo (potrebbero esserci dei parimerito)\n",
    "        - Normalized Discounted Cumulative Gain con taglio $\\text{K} = \\set{5,15}$:\n",
    "            \n",
    "            $$\n",
    "            \\text{NDCG@K}_q = \\frac{\\text{DCG@K}_q}{\\text{IDCG@K}_q}\n",
    "            $$\n",
    "            \n",
    "            - dove dividiamo il $\\text{DCG@K}_q = rel_1 + \\sum_{i=2}^K \\frac{rel_i}{\\log_2 (i + 1)}$ con quello ideale, cioè dove il ranking è il migliore possibile\n",
    "4. Calcolare la media delle metriche:\n",
    "    - Mean Reciprocal Rank: $\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{RR}_q$\n",
    "    - Media dei NDCG: $\\frac{1}{|Q|} \\sum_{q \\in Q} \\text{NDCG@K}_q$\n",
    "\n",
    "### Query (in verde stesso ranking ma proviamo sinonimi)\n",
    "\n",
    "1. NDCG su dataset movielens ✅\n",
    "2. Recommender systems Recall su dataset goodbook ✅\n",
    "3. Recommender systems MRR ✅\n",
    "4. Deep Learning dataset Apple Flower ✅\n",
    "5. Deep Learning GPT3 precision f1 ✅\n",
    "6. Deep Learning GPT3 precision f-measure ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "results_file = \"./results.json\"\n",
    "ground_truth_path = \"./ground_truth\"\n",
    "num_queries = 6\n",
    "\n",
    "# model -> method -> query -> (position, id) \n",
    "results: dict[str, dict[str, dict[str, dict[str, str]]]] = {}\n",
    "\n",
    "# query -> (position, table[table_id, query_id, rel]) \n",
    "ground_truth: dict[str, dict[str, dict[str, str]]] = {}\n",
    "\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "    \n",
    "for i in range(1, num_queries + 1):\n",
    "    query_id = f\"q{i}\"\n",
    "    with open(ground_truth_path + f\"/{query_id}_rank.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        ground_truth[query_id] = json.load(file)\n",
    "            \n",
    "\n",
    "def compare_id(id1: str, id2: str) -> bool:\n",
    "    id1 = str.lower(re.sub(r'v\\d+', '', id1))\n",
    "    id2 = str.lower(re.sub(r'v\\d+', '', id2))\n",
    "\n",
    "    return id1 == id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value for lucene using method: bm25 is 0.5183760683760684 --- best not found in 0/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_embedding is 0.09861111111111111 --- best not found in 2/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_embedding is 0.09907407407407408 --- best not found in 2/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_ref_embedding is 0.11282051282051281 --- best not found in 3/6 queries.\n",
      "MRR value for bert-base-uncased using method: weighted_embedding is 0.16625966625966623 --- best not found in 1/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_embedding is 0.07181522181522182 --- best not found in 2/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_embedding is 0.10833333333333332 --- best not found in 1/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.12222222222222222 --- best not found in 3/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: weighted_embedding is 0.13293650793650794 --- best not found in 1/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.034126984126984124 --- best not found in 3/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.06468253968253969 --- best not found in 2/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.05833333333333333 --- best not found in 3/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.08749999999999998 --- best not found in 1/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_embedding is 0.225 --- best not found in 3/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_cap_embedding is 0.24754273504273502 --- best not found in 0/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_cap_ref_embedding is 0.48305860805860806 --- best not found in 0/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: weighted_embedding is 0.27904040404040403 --- best not found in 1/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_embedding is 0.17380952380952383 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_cap_embedding is 0.31467236467236465 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_cap_ref_embedding is 0.2957070707070707 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: weighted_embedding is 0.33888888888888885 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_embedding is 0.24523809523809523 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_embedding is 0.4488095238095238 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_ref_embedding is 0.43504273504273505 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: weighted_embedding is 0.6 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_embedding is 0.325 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_embedding is 0.3611111111111111 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_ref_embedding is 0.29444444444444445 --- best not found in 0/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: weighted_embedding is 0.36408730158730157 --- best not found in 0/6 queries.\n"
     ]
    }
   ],
   "source": [
    "mrr_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    mrr_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_rr = 0\n",
    "        not_founds = 0\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            best_tables_ids: list[str] = []\n",
    "            best_table =  ground_truth[query_id][\"1\"]\n",
    "            \n",
    "            # this checks for equal relevance tables other than the first position\n",
    "            for pos, table in ground_truth[query_id].items():\n",
    "                if table[\"rel\"] == best_table[\"rel\"]:\n",
    "                    best_tables_ids.append(table[\"paper_id\"] + \"#\" + table[\"table_id\"])\n",
    "            \n",
    "            rr = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                for best_table_id in best_tables_ids:\n",
    "                    if (compare_id(best_table_id, table_id)):\n",
    "                        rr = 1.0 / float(pos)\n",
    "            \n",
    "            if rr == 0: not_founds += 1\n",
    "            sum_rr += rr\n",
    "            \n",
    "        mrr = sum_rr / num_queries\n",
    "        mrr_values[model][method] = mrr\n",
    "        print(f\"MRR value for {model} using method: {method} is {mrr} --- best not found in {not_founds}/{num_queries} queries.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== lucene --> bm25 ==============\n",
      "Average NDCG@15 is 0.6773112397486053.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.4512641678598764.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.41729195009032055.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.3935216966623996.\n",
      "\n",
      "\n",
      "============== bert-base-uncased --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.4959938484056883.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.48094101887902413.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.44065978032622705.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.322777790941915.\n",
      "\n",
      "\n",
      "============== distilbert-base-uncased --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.498106835389123.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.3609988244131619.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.32767096540570656.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.24046849028324516.\n",
      "\n",
      "\n",
      "============== allenai/scibert_scivocab_uncased --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.35815160053804124.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.6713636592616309.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.5878167791127177.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.6115845500774436.\n",
      "\n",
      "\n",
      "============== all-mpnet-base-v2 --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.6658750299015253.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.7438867141259576.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.6662187721234846.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.7588920883252245.\n",
      "\n",
      "\n",
      "============== sentence-transformers/sentence-t5-large --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.7256463563176393.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.72845274614367.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.5939026249359005.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.6880181765514459.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L6-v2 --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.7142032082276635.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_embedding ==============\n",
      "Average NDCG@15 is 0.7824008800959704.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_cap_embedding ==============\n",
      "Average NDCG@15 is 0.5877362948587758.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_cap_ref_embedding ==============\n",
      "Average NDCG@15 is 0.7974026574348233.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> weighted_embedding ==============\n",
      "Average NDCG@15 is 0.7491345777153064.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "K = 15\n",
    "idcg_values: dict[str, float] = {}\n",
    "\n",
    "for query_id, ranking in ground_truth.items():    \n",
    "    idcg = 0\n",
    "\n",
    "    for i in range(1, K + 1):\n",
    "        table = ground_truth[query_id][str(i)]\n",
    "        table_id = table[\"paper_id\"] + \"#\" + table[\"table_id\"]\n",
    "        rel: float = float(table[\"rel\"])\n",
    "\n",
    "        idcg += rel / math.log2(i + 1)\n",
    "    \n",
    "    idcg_values[query_id] = idcg\n",
    "\n",
    "for model, methods in results.items():\n",
    "    for method, queries in methods.items():\n",
    "        print(f\"============== {model} --> {method} ==============\")\n",
    "        sum_ndcg: float = 0\n",
    "        for query_id, ranking in queries.items():\n",
    "            dcg = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                rel: float = 0\n",
    "                \n",
    "                for _, gt_table in ground_truth[query_id].items():\n",
    "                   gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                   if compare_id(table_id, gt_table_id):\n",
    "                       rel = float(gt_table[\"rel\"])\n",
    "                \n",
    "                dcg += rel / math.log2(int(pos) + 1)\n",
    "                \n",
    "            ndcg = dcg / idcg_values[query_id] if dcg / idcg_values[query_id] <= 1 else 1\n",
    "            sum_ndcg += ndcg\n",
    "            \n",
    "            #print(f\"NDCG@{K} for query {query_id} is {ndcg}.\")\n",
    "        print(f\"Average NDCG@{K} is {sum_ndcg / num_queries}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Precision@15 of lucene with bm25 for query q1 is: 0.7452438302438302\n",
      "Avg Precision@15 of lucene with bm25 for query q2 is: 0.6194063344063344\n",
      "Avg Precision@15 of lucene with bm25 for query q3 is: 0.6380587930587931\n",
      "Avg Precision@15 of lucene with bm25 for query q4 is: 0.8356745106745105\n",
      "Avg Precision@15 of lucene with bm25 for query q5 is: 0.7924917674917674\n",
      "Avg Precision@15 of lucene with bm25 for query q6 is: 0.8067822917822919\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q1 is: 0.5670510970510971\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q2 is: 0.5687747437747438\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q3 is: 0.18678987678987677\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q4 is: 0.46575720575720575\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q5 is: 0.6282985532985533\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q6 is: 0.6052562252562254\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q1 is: 0.6989012839012839\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q2 is: 0.5650669700669702\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q3 is: 0.1573759573759574\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q4 is: 0.5140352240352241\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q5 is: 0.2793567543567544\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q6 is: 0.632022977022977\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q1 is: 0.5134260184260184\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q2 is: 0.7392740592740593\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q3 is: 0.3282713582713582\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q4 is: 0.3066352166352166\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q5 is: 0.3535359085359085\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q6 is: 0.5007076257076256\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q1 is: 0.766070596070596\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q2 is: 0.736180486180486\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q3 is: 0.15422540422540423\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q4 is: 0.44678414178414183\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q5 is: 0.556998186998187\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q6 is: 0.6721328671328671\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q1 is: 0.5991956191956193\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q2 is: 0.45181226181226186\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q3 is: 0.1656811706811707\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q4 is: 0.6341699041699042\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q5 is: 0.7022122322122323\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q6 is: 0.6119228919228921\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q1 is: 0.655966440966441\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q2 is: 0.4701768601768601\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q3 is: 0.14404021904021905\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q4 is: 0.5919981869981871\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q5 is: 0.4634508084508085\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q6 is: 0.603925333925334\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q1 is: 0.6977901727901727\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q2 is: 0.6879000629000629\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q3 is: 0.3685912235912235\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q4 is: 0.14255873755873757\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q5 is: 0.19565397565397566\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q6 is: 0.18856643356643357\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q1 is: 0.7498312798312797\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q2 is: 0.6975826025826026\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q3 is: 0.10940651940651941\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q4 is: 0.4895130795130796\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q5 is: 0.5697759647759647\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q6 is: 0.6485838235838236\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q1 is: 0.44890701890701895\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q2 is: 0.4003124653124653\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q3 is: 0.14533892033892035\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q4 is: 0.5192228142228142\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q5 is: 0.45451714951714944\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q6 is: 0.33012321012321005\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q1 is: 0.6447502497502497\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q2 is: 0.42023310023310023\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q3 is: 0.11822418322418322\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q4 is: 0.3052578902578903\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q5 is: 0.4366294816294815\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q6 is: 0.37301087801087796\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q1 is: 0.46697339697339696\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q2 is: 0.46414511414511417\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q3 is: 0.16417489917489916\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q4 is: 0.15475616975616976\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q5 is: 0.29475690975690977\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q6 is: 0.1733018833018833\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q1 is: 0.6121328671328672\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q2 is: 0.4092733192733193\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q3 is: 0.13046712546712547\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q4 is: 0.32790172790172795\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q5 is: 0.5220287120287119\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q6 is: 0.3345171495171495\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q1 is: 0.6654323454323453\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q2 is: 0.9096899396899398\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q3 is: 0.8613318163318162\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q4 is: 0.7086937136937137\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q5 is: 0.6312045362045361\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q6 is: 0.4970246420246421\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q1 is: 0.7683647833647833\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q2 is: 0.7184804084804085\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q3 is: 0.6029248529248529\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q4 is: 0.7357531357531357\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q5 is: 0.5942492692492694\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q6 is: 0.600911865911866\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q1 is: 0.7054828504828504\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q2 is: 0.9530480630480631\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q3 is: 0.789188959188959\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q4 is: 0.7358630258630259\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q5 is: 0.4964674214674215\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q6 is: 0.26969992969992973\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q1 is: 0.8350908350908349\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q2 is: 0.8395857845857844\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q3 is: 0.7215145965145964\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q4 is: 0.7864657564657562\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q5 is: 0.6115244015244016\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q6 is: 0.4511003811003811\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q1 is: 0.7706175306175304\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q2 is: 0.6769948569948568\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q3 is: 0.8689980389980392\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q4 is: 0.7671817071817072\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q5 is: 0.6884027084027085\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q6 is: 0.7181829281829282\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q1 is: 0.8371807821807821\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q2 is: 0.7169989269989271\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q3 is: 0.7325222925222923\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q4 is: 0.7099089799089798\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q5 is: 0.5905984755984756\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q6 is: 0.6110152810152809\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q1 is: 0.8306952306952307\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q2 is: 0.9134465534465535\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q3 is: 0.6941071891071889\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q4 is: 0.7621312021312021\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q5 is: 0.7442740592740594\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q6 is: 0.7875727975727974\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q1 is: 0.8941327191327191\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q2 is: 0.583105968105968\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q3 is: 0.8233399933399931\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q4 is: 0.8051735301735302\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q5 is: 0.7206432456432456\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q6 is: 0.6917038517038516\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q1 is: 0.6128397528397528\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q2 is: 0.7918080068080066\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q3 is: 0.8044559144559144\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q4 is: 0.7042492692492692\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q5 is: 0.8558251008251007\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q6 is: 0.752124727124727\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q1 is: 0.8496882746882748\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q2 is: 0.6018130018130018\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q3 is: 0.8245633995633994\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q4 is: 0.736180486180486\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q5 is: 0.6532134532134533\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q6 is: 0.6018377918377918\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q1 is: 0.7903224553224553\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q2 is: 0.816252451252451\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q3 is: 0.5664898064898066\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q4 is: 0.7413110963110963\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q5 is: 0.7184804084804085\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q6 is: 0.6736086136086137\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q1 is: 0.7929174529174529\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q2 is: 0.7870990120990118\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q3 is: 0.6168601768601768\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q4 is: 0.7183233433233434\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q5 is: 0.758077848077848\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q6 is: 0.7378149628149627\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q1 is: 0.7138938838938839\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q2 is: 0.8200843600843599\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q3 is: 0.895849890849891\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q4 is: 0.7109159359159359\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q5 is: 0.82506364006364\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q6 is: 0.831124246124246\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q1 is: 0.858894623894624\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q2 is: 0.44866152366152373\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q3 is: 0.7763301513301513\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q4 is: 0.6295130795130796\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q5 is: 0.5824190624190624\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q6 is: 0.602924112924113\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q1 is: 0.7383383283383282\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q2 is: 0.8482043882043884\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q3 is: 0.8801091501091503\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q4 is: 0.7266566766566768\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q5 is: 0.8650884300884301\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q6 is: 0.8448808598808597\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q1 is: 0.8202166352166353\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q2 is: 0.8516981166981168\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q3 is: 0.6634193584193583\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q4 is: 0.7621312021312021\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q5 is: 0.7423123173123173\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q6 is: 0.7049330299330298\n"
     ]
    }
   ],
   "source": [
    "# Precision at k\n",
    "def precision_at_k(k: int):\n",
    "    precision_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "    for model, methods in results.items():\n",
    "        precision_values[model] = {}\n",
    "        for method, queries in methods.items():\n",
    "            precision_values[model][method] = {}\n",
    "            for query_id, ranking in queries.items():\n",
    "                relevant = 0\n",
    "                for pos, table_id in ranking.items():\n",
    "                    if int(pos) > k: break\n",
    "                    \n",
    "                    rel: float = 0\n",
    "                    \n",
    "                    # find relevance\n",
    "                    for _, gt_table in ground_truth[query_id].items():\n",
    "                        gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                        if compare_id(table_id, gt_table_id):\n",
    "                            rel = float(gt_table[\"rel\"])\n",
    "                    \n",
    "                    if rel > 0: relevant += 1\n",
    "                \n",
    "                precision = relevant / k\n",
    "                precision_values[model][method][query_id] = precision\n",
    "    \n",
    "    return precision_values\n",
    "\n",
    "# Avg Precision at K\n",
    "K = 15\n",
    "# model -> method -> query, avg_precision@k\n",
    "ap_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    ap_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        ap_values[model][method] = {}\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            sum_p = 0\n",
    "            \n",
    "            for k in range(1, K + 1):\n",
    "                precision_values_at_k = precision_at_k(k)\n",
    "                sum_p += precision_values_at_k[model][method][query_id]\n",
    "                \n",
    "            ap_values[model][method][query_id] = sum_p / K\n",
    "            print(f\"Avg Precision@{K} of {model} with {method} for query {query_id} is: {ap_values[model][method][query_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@15 for lucene using method: bm25 is 0.7396095879429213.\n",
      "MAP@15 for bert-base-uncased using method: tab_embedding is 0.5036546169879504.\n",
      "MAP@15 for bert-base-uncased using method: tab_cap_embedding is 0.4744598611265278.\n",
      "MAP@15 for bert-base-uncased using method: tab_cap_ref_embedding is 0.4569750311416978.\n",
      "MAP@15 for bert-base-uncased using method: weighted_embedding is 0.555398613731947.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_embedding is 0.5274990133323468.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_cap_embedding is 0.4882596415929749.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.3801767676767676.\n",
      "MAP@15 for distilbert-base-uncased using method: weighted_embedding is 0.5441155449488784.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.383070263070263.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.38301763051763055.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.2863513955180622.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.38938681688681687.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_embedding is 0.7122294988961656.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_cap_embedding is 0.6701140526140525.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_cap_ref_embedding is 0.6582917082917082.\n",
      "MAP@15 for all-mpnet-base-v2 using method: weighted_embedding is 0.7075469592136256.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_embedding is 0.7483962950629617.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_cap_embedding is 0.6997041230374563.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_cap_ref_embedding is 0.788704505371172.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: weighted_embedding is 0.7530165513498847.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_embedding is 0.753550461883795.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_embedding is 0.7112160678827345.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_ref_embedding is 0.717744138577472.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: weighted_embedding is 0.7351821326821325.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_embedding is 0.7994886594886594.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_embedding is 0.649790425623759.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_ref_embedding is 0.8172129722129723.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: weighted_embedding is 0.7574517766184433.\n"
     ]
    }
   ],
   "source": [
    "# MAP@K (K è quello sopra)\n",
    "map_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    map_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_ap = 0\n",
    "        \n",
    "        for avg_prec in ap_values[model][method].values():\n",
    "            sum_ap += avg_prec\n",
    "        \n",
    "        map_value = sum_ap / num_queries\n",
    "        map_values[model][method] = map_value\n",
    "            \n",
    "        print(f\"MAP@{K} for {model} using method: {method} is {map_value}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
