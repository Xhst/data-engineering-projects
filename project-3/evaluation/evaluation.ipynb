{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione\n",
    "\n",
    "1. Creare tot sistemi diversi (per esempio, lucene, bert, scibert, tabert (con e senza contesto))\n",
    "2. Creare un sottoinsieme dei paper (tipo 20) da usare come ground truth (o a caso oppure con lucene i più rilevanti per argomento che abbiano tabelle interessanti) - cerchiamo di limitare il numero di tabelle a ca. 50\n",
    "3. Per ogni query $q \\in Q$ (min 5):\n",
    "    1. Fare il ranking a mano delle tabelle\n",
    "        - Salviamo i ranking per ogni query in un json, con le informazioni rilevanti, tipo il ranking, il valore di rilevanza per ogni elemento etc.\n",
    "    2. Interrogare ogni sistema sulla query\n",
    "    3. Calcolare le metriche: \n",
    "        - Reciprocal Rank: $\\text{RR}_q = \\frac{1}{rank_i}$ dove $i$ è l’elemento più rilevante.\n",
    "            - nella pratica possiamo controllare se l’elemento scelto dal motore ha almeno lo score massimo (potrebbero esserci dei parimerito)\n",
    "        - Normalized Discounted Cumulative Gain con taglio $\\text{K} = \\set{5,15}$:\n",
    "            \n",
    "            $$\n",
    "            \\text{NDCG@K}_q = \\frac{\\text{DCG@K}_q}{\\text{IDCG@K}_q}\n",
    "            $$\n",
    "            \n",
    "            - dove dividiamo il $\\text{DCG@K}_q = rel_1 + \\sum_{i=2}^K \\frac{rel_i}{\\log_2 (i + 1)}$ con quello ideale, cioè dove il ranking è il migliore possibile\n",
    "4. Calcolare la media delle metriche:\n",
    "    - Mean Reciprocal Rank: $\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{RR}_q$\n",
    "    - Media dei NDCG: $\\frac{1}{|Q|} \\sum_{q \\in Q} \\text{NDCG@K}_q$\n",
    "\n",
    "### Query (in verde stesso ranking ma proviamo sinonimi)\n",
    "\n",
    "1. NDCG su dataset movielens ✅\n",
    "2. Recommender systems Recall su dataset goodbook ✅\n",
    "3. Recommender systems MRR ✅\n",
    "4. Deep Learning dataset Apple Flower ✅\n",
    "5. Deep Learning GPT3 precision f1 ✅\n",
    "6. Deep Learning GPT3 precision f-measure ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_file = \"./results.json\"\n",
    "ground_truth_path = \"./ground_truth\"\n",
    "num_queries = 6\n",
    "\n",
    "# model -> method -> query -> (position, id) \n",
    "results: dict[str, dict[str, dict[str, dict[str, str]]]] = {}\n",
    "\n",
    "# query -> (position, table[table_id, query_id, rel]) \n",
    "ground_truth: dict[str, dict[str, dict[str, str]]] = {}\n",
    "\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "    \n",
    "for i in range(1, num_queries + 1):\n",
    "    query_id = f\"q{i}\"\n",
    "    with open(ground_truth_path + f\"/{query_id}_rank.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        ground_truth[query_id] = json.load(file)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value for lucene using method: bm25 is 0.03333333333333333 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_embedding is 0.016666666666666666 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: tab_cap_ref_embedding is 0.016666666666666666 --- best not found in 5/6 queries.\n",
      "MRR value for bert-base-uncased using method: weighted_embedding is 0.018518518518518517 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_embedding is 0.018518518518518517 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_embedding is 0.020833333333333332 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n",
      "MRR value for distilbert-base-uncased using method: weighted_embedding is 0.013888888888888888 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.011904761904761904 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n",
      "MRR value for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.011111111111111112 --- best not found in 5/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_cap_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: tab_cap_ref_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for all-mpnet-base-v2 using method: weighted_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_cap_embedding is 0.03333333333333333 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: tab_cap_ref_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/sentence-t5-large using method: weighted_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_embedding is 0.03333333333333333 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_ref_embedding is 0.03333333333333333 --- best not found in 5/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L6-v2 using method: weighted_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for deepset/sentence_bert using method: tab_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for deepset/sentence_bert using method: tab_cap_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for deepset/sentence_bert using method: tab_cap_ref_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n",
      "MRR value for deepset/sentence_bert using method: weighted_embedding is 0.041666666666666664 --- best not found in 5/6 queries.\n"
     ]
    }
   ],
   "source": [
    "mrr_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    mrr_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_rr = 0\n",
    "        not_founds = 0\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            best_tables_ids: list[str] = []\n",
    "            best_table =  ground_truth[query_id][\"1\"]\n",
    "            \n",
    "            # this checks for equal relevance tables other than the first position\n",
    "            for pos, table in ground_truth[query_id].items():\n",
    "                if table[\"rel\"] == best_table[\"rel\"]:\n",
    "                    best_tables_ids.append(table[\"paper_id\"] + \"#\" + table[\"table_id\"])\n",
    "            \n",
    "            rr = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                if (table_id in best_tables_ids): rr = 1.0 / float(pos)\n",
    "            \n",
    "            if rr == 0: not_founds += 1\n",
    "            sum_rr += rr\n",
    "            \n",
    "        mrr = sum_rr / num_queries\n",
    "        mrr_values[model][method] = mrr\n",
    "        print(f\"MRR value for {model} using method: {method} is {mrr} --- best not found in {not_founds}/{num_queries} queries.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for query q1 using lucene with method bm25 is 0.0490282252127475.\n",
      "NDCG@5 for query q2 using lucene with method bm25 is 0.09190425572979682.\n",
      "NDCG@5 for query q3 using lucene with method bm25 is 0.024488957205226678.\n",
      "NDCG@5 for query q4 using lucene with method bm25 is 0.8736845820892508.\n",
      "NDCG@5 for query q5 using lucene with method bm25 is 0.12700516190404718.\n",
      "NDCG@5 for query q6 using lucene with method bm25 is 0.17349138312049142.\n",
      "NDCG@5 for query q1 using bert-base-uncased with method tab_embedding is 0.10600612031665564.\n",
      "NDCG@5 for query q2 using bert-base-uncased with method tab_embedding is 0.2613259601900334.\n",
      "NDCG@5 for query q3 using bert-base-uncased with method tab_embedding is 0.05456251053556509.\n",
      "NDCG@5 for query q4 using bert-base-uncased with method tab_embedding is 0.6592170886358156.\n",
      "NDCG@5 for query q5 using bert-base-uncased with method tab_embedding is 0.028165898063816423.\n",
      "NDCG@5 for query q6 using bert-base-uncased with method tab_embedding is 0.0.\n",
      "NDCG@5 for query q1 using bert-base-uncased with method tab_cap_embedding is 0.13249643973411912.\n",
      "NDCG@5 for query q2 using bert-base-uncased with method tab_cap_embedding is 0.2463364981800553.\n",
      "NDCG@5 for query q3 using bert-base-uncased with method tab_cap_embedding is 0.04878249642476229.\n",
      "NDCG@5 for query q4 using bert-base-uncased with method tab_cap_embedding is 0.7441738916577658.\n",
      "NDCG@5 for query q5 using bert-base-uncased with method tab_cap_embedding is 0.05959261235513805.\n",
      "NDCG@5 for query q6 using bert-base-uncased with method tab_cap_embedding is 0.07912563318612227.\n",
      "NDCG@5 for query q1 using bert-base-uncased with method tab_cap_ref_embedding is 0.21301798490618176.\n",
      "NDCG@5 for query q2 using bert-base-uncased with method tab_cap_ref_embedding is 0.3009208026430688.\n",
      "NDCG@5 for query q3 using bert-base-uncased with method tab_cap_ref_embedding is 0.107787789834017.\n",
      "NDCG@5 for query q4 using bert-base-uncased with method tab_cap_ref_embedding is 0.5743348846981837.\n",
      "NDCG@5 for query q5 using bert-base-uncased with method tab_cap_ref_embedding is 0.0.\n",
      "NDCG@5 for query q6 using bert-base-uncased with method tab_cap_ref_embedding is 0.03142671429132163.\n",
      "NDCG@5 for query q1 using bert-base-uncased with method weighted_embedding is 0.19081101656998015.\n",
      "NDCG@5 for query q2 using bert-base-uncased with method weighted_embedding is 0.2740405310137595.\n",
      "NDCG@5 for query q3 using bert-base-uncased with method weighted_embedding is 0.055573552093997294.\n",
      "NDCG@5 for query q4 using bert-base-uncased with method weighted_embedding is 0.7800390269710875.\n",
      "NDCG@5 for query q5 using bert-base-uncased with method weighted_embedding is 0.029591040121465493.\n",
      "NDCG@5 for query q6 using bert-base-uncased with method weighted_embedding is 0.03391512068809118.\n",
      "NDCG@5 for query q1 using distilbert-base-uncased with method tab_embedding is 0.11794005047129913.\n",
      "NDCG@5 for query q2 using distilbert-base-uncased with method tab_embedding is 0.17572290128294227.\n",
      "NDCG@5 for query q3 using distilbert-base-uncased with method tab_embedding is 0.04669349224141275.\n",
      "NDCG@5 for query q4 using distilbert-base-uncased with method tab_embedding is 0.9149322567986564.\n",
      "NDCG@5 for query q5 using distilbert-base-uncased with method tab_embedding is 0.04852156813268202.\n",
      "NDCG@5 for query q6 using distilbert-base-uncased with method tab_embedding is 0.043584226937077294.\n",
      "NDCG@5 for query q1 using distilbert-base-uncased with method tab_cap_embedding is 0.13045924728601574.\n",
      "NDCG@5 for query q2 using distilbert-base-uncased with method tab_cap_embedding is 0.07844182413960506.\n",
      "NDCG@5 for query q3 using distilbert-base-uncased with method tab_cap_embedding is 0.04425697094631987.\n",
      "NDCG@5 for query q4 using distilbert-base-uncased with method tab_cap_embedding is 0.8039971958825866.\n",
      "NDCG@5 for query q5 using distilbert-base-uncased with method tab_cap_embedding is 0.03044600124221507.\n",
      "NDCG@5 for query q6 using distilbert-base-uncased with method tab_cap_embedding is 0.10250952678941157.\n",
      "NDCG@5 for query q1 using distilbert-base-uncased with method tab_cap_ref_embedding is 0.21301798490618176.\n",
      "NDCG@5 for query q2 using distilbert-base-uncased with method tab_cap_ref_embedding is 0.3009208026430688.\n",
      "NDCG@5 for query q3 using distilbert-base-uncased with method tab_cap_ref_embedding is 0.10945899318069663.\n",
      "NDCG@5 for query q4 using distilbert-base-uncased with method tab_cap_ref_embedding is 0.2778362670650681.\n",
      "NDCG@5 for query q5 using distilbert-base-uncased with method tab_cap_ref_embedding is 0.0.\n",
      "NDCG@5 for query q6 using distilbert-base-uncased with method tab_cap_ref_embedding is 0.0.\n",
      "NDCG@5 for query q1 using distilbert-base-uncased with method weighted_embedding is 0.1399293770207598.\n",
      "NDCG@5 for query q2 using distilbert-base-uncased with method weighted_embedding is 0.18358805282195853.\n",
      "NDCG@5 for query q3 using distilbert-base-uncased with method weighted_embedding is 0.062095480094078635.\n",
      "NDCG@5 for query q4 using distilbert-base-uncased with method weighted_embedding is 0.7281508186428095.\n",
      "NDCG@5 for query q5 using distilbert-base-uncased with method weighted_embedding is 0.029591040121465493.\n",
      "NDCG@5 for query q6 using distilbert-base-uncased with method weighted_embedding is 0.040131581286733345.\n",
      "NDCG@5 for query q1 using allenai/scibert_scivocab_uncased with method tab_embedding is 0.05300306015832782.\n",
      "NDCG@5 for query q2 using allenai/scibert_scivocab_uncased with method tab_embedding is 0.18213627528396267.\n",
      "NDCG@5 for query q3 using allenai/scibert_scivocab_uncased with method tab_embedding is 0.024488957205226678.\n",
      "NDCG@5 for query q4 using allenai/scibert_scivocab_uncased with method tab_embedding is 0.7801193201144924.\n",
      "NDCG@5 for query q5 using allenai/scibert_scivocab_uncased with method tab_embedding is 0.03554140624904497.\n",
      "NDCG@5 for query q6 using allenai/scibert_scivocab_uncased with method tab_embedding is 0.03391512068809118.\n",
      "NDCG@5 for query q1 using allenai/scibert_scivocab_uncased with method tab_cap_embedding is 0.15706375541950096.\n",
      "NDCG@5 for query q2 using allenai/scibert_scivocab_uncased with method tab_cap_embedding is 0.1149151952813337.\n",
      "NDCG@5 for query q3 using allenai/scibert_scivocab_uncased with method tab_cap_embedding is 0.0.\n",
      "NDCG@5 for query q4 using allenai/scibert_scivocab_uncased with method tab_cap_embedding is 0.31015831722987686.\n",
      "NDCG@5 for query q5 using allenai/scibert_scivocab_uncased with method tab_cap_embedding is 0.03044600124221507.\n",
      "NDCG@5 for query q6 using allenai/scibert_scivocab_uncased with method tab_cap_embedding is 0.0.\n",
      "NDCG@5 for query q1 using allenai/scibert_scivocab_uncased with method tab_cap_ref_embedding is 0.1399293770207598.\n",
      "NDCG@5 for query q2 using allenai/scibert_scivocab_uncased with method tab_cap_ref_embedding is 0.19953900000641986.\n",
      "NDCG@5 for query q3 using allenai/scibert_scivocab_uncased with method tab_cap_ref_embedding is 0.07118244944663941.\n",
      "NDCG@5 for query q4 using allenai/scibert_scivocab_uncased with method tab_cap_ref_embedding is 0.18110561822519322.\n",
      "NDCG@5 for query q5 using allenai/scibert_scivocab_uncased with method tab_cap_ref_embedding is 0.0.\n",
      "NDCG@5 for query q6 using allenai/scibert_scivocab_uncased with method tab_cap_ref_embedding is 0.0.\n",
      "NDCG@5 for query q1 using allenai/scibert_scivocab_uncased with method weighted_embedding is 0.13249643973411912.\n",
      "NDCG@5 for query q2 using allenai/scibert_scivocab_uncased with method weighted_embedding is 0.17572290128294227.\n",
      "NDCG@5 for query q3 using allenai/scibert_scivocab_uncased with method weighted_embedding is 0.06115424153999472.\n",
      "NDCG@5 for query q4 using allenai/scibert_scivocab_uncased with method weighted_embedding is 0.4169578888277804.\n",
      "NDCG@5 for query q5 using allenai/scibert_scivocab_uncased with method weighted_embedding is 0.03391512068809118.\n",
      "NDCG@5 for query q6 using allenai/scibert_scivocab_uncased with method weighted_embedding is 0.0.\n",
      "NDCG@5 for query q1 using all-mpnet-base-v2 with method tab_embedding is 0.0.\n",
      "NDCG@5 for query q2 using all-mpnet-base-v2 with method tab_embedding is 0.05745759764066685.\n",
      "NDCG@5 for query q3 using all-mpnet-base-v2 with method tab_embedding is 0.041036791325713994.\n",
      "NDCG@5 for query q4 using all-mpnet-base-v2 with method tab_embedding is 1.0378665954409996.\n",
      "NDCG@5 for query q5 using all-mpnet-base-v2 with method tab_embedding is 0.09598272141485696.\n",
      "NDCG@5 for query q6 using all-mpnet-base-v2 with method tab_embedding is 0.0995352022679843.\n",
      "NDCG@5 for query q1 using all-mpnet-base-v2 with method tab_cap_embedding is 0.06041622709476875.\n",
      "NDCG@5 for query q2 using all-mpnet-base-v2 with method tab_cap_embedding is 0.07494469257478287.\n",
      "NDCG@5 for query q3 using all-mpnet-base-v2 with method tab_cap_embedding is 0.04403420037034134.\n",
      "NDCG@5 for query q4 using all-mpnet-base-v2 with method tab_cap_embedding is 1.0170553927603128.\n",
      "NDCG@5 for query q5 using all-mpnet-base-v2 with method tab_cap_embedding is 0.12797838460619942.\n",
      "NDCG@5 for query q6 using all-mpnet-base-v2 with method tab_cap_embedding is 0.25794065339267425.\n",
      "NDCG@5 for query q1 using all-mpnet-base-v2 with method tab_cap_ref_embedding is 0.14234723802841134.\n",
      "NDCG@5 for query q2 using all-mpnet-base-v2 with method tab_cap_ref_embedding is 0.17624212499977296.\n",
      "NDCG@5 for query q3 using all-mpnet-base-v2 with method tab_cap_ref_embedding is 0.058797442018233385.\n",
      "NDCG@5 for query q4 using all-mpnet-base-v2 with method tab_cap_ref_embedding is 1.0747835120277138.\n",
      "NDCG@5 for query q5 using all-mpnet-base-v2 with method tab_cap_ref_embedding is 0.07749934762516848.\n",
      "NDCG@5 for query q6 using all-mpnet-base-v2 with method tab_cap_ref_embedding is 0.165871118673102.\n",
      "NDCG@5 for query q1 using all-mpnet-base-v2 with method weighted_embedding is 0.08303812758138025.\n",
      "NDCG@5 for query q2 using all-mpnet-base-v2 with method weighted_embedding is 0.07844182413960506.\n",
      "NDCG@5 for query q3 using all-mpnet-base-v2 with method weighted_embedding is 0.021383619542205468.\n",
      "NDCG@5 for query q4 using all-mpnet-base-v2 with method weighted_embedding is 1.0921976854430808.\n",
      "NDCG@5 for query q5 using all-mpnet-base-v2 with method weighted_embedding is 0.07269866301435106.\n",
      "NDCG@5 for query q6 using all-mpnet-base-v2 with method weighted_embedding is 0.13132792253863182.\n",
      "NDCG@5 for query q1 using sentence-transformers/sentence-t5-large with method tab_embedding is 0.04240244812666226.\n",
      "NDCG@5 for query q2 using sentence-transformers/sentence-t5-large with method tab_embedding is 0.18643060679305323.\n",
      "NDCG@5 for query q3 using sentence-transformers/sentence-t5-large with method tab_embedding is 0.07364331464172705.\n",
      "NDCG@5 for query q4 using sentence-transformers/sentence-t5-large with method tab_embedding is 1.0.\n",
      "NDCG@5 for query q5 using sentence-transformers/sentence-t5-large with method tab_embedding is 0.03755453075175523.\n",
      "NDCG@5 for query q6 using sentence-transformers/sentence-t5-large with method tab_embedding is 0.06990072140416907.\n",
      "NDCG@5 for query q1 using sentence-transformers/sentence-t5-large with method tab_cap_embedding is 0.0.\n",
      "NDCG@5 for query q2 using sentence-transformers/sentence-t5-large with method tab_cap_embedding is 0.0.\n",
      "NDCG@5 for query q3 using sentence-transformers/sentence-t5-large with method tab_cap_embedding is 0.024488957205226678.\n",
      "NDCG@5 for query q4 using sentence-transformers/sentence-t5-large with method tab_cap_embedding is 0.8764159884418177.\n",
      "NDCG@5 for query q5 using sentence-transformers/sentence-t5-large with method tab_cap_embedding is 0.10762271453733384.\n",
      "NDCG@5 for query q6 using sentence-transformers/sentence-t5-large with method tab_cap_embedding is 0.15366704798335723.\n",
      "NDCG@5 for query q1 using sentence-transformers/sentence-t5-large with method tab_cap_ref_embedding is 0.0.\n",
      "NDCG@5 for query q2 using sentence-transformers/sentence-t5-large with method tab_cap_ref_embedding is 0.17361300951809108.\n",
      "NDCG@5 for query q3 using sentence-transformers/sentence-t5-large with method tab_cap_ref_embedding is 0.03789003686737222.\n",
      "NDCG@5 for query q4 using sentence-transformers/sentence-t5-large with method tab_cap_ref_embedding is 1.0.\n",
      "NDCG@5 for query q5 using sentence-transformers/sentence-t5-large with method tab_cap_ref_embedding is 0.03142671429132163.\n",
      "NDCG@5 for query q6 using sentence-transformers/sentence-t5-large with method tab_cap_ref_embedding is 0.11174966359569954.\n",
      "NDCG@5 for query q1 using sentence-transformers/sentence-t5-large with method weighted_embedding is 0.0.\n",
      "NDCG@5 for query q2 using sentence-transformers/sentence-t5-large with method weighted_embedding is 0.1354294338627198.\n",
      "NDCG@5 for query q3 using sentence-transformers/sentence-t5-large with method weighted_embedding is 0.08210652971575563.\n",
      "NDCG@5 for query q4 using sentence-transformers/sentence-t5-large with method weighted_embedding is 0.9885057531265261.\n",
      "NDCG@5 for query q5 using sentence-transformers/sentence-t5-large with method weighted_embedding is 0.1326529926338965.\n",
      "NDCG@5 for query q6 using sentence-transformers/sentence-t5-large with method weighted_embedding is 0.13700402327439273.\n",
      "NDCG@5 for query q1 using sentence-transformers/all-MiniLM-L6-v2 with method tab_embedding is 0.03776014193423047.\n",
      "NDCG@5 for query q2 using sentence-transformers/all-MiniLM-L6-v2 with method tab_embedding is 0.13783408781915812.\n",
      "NDCG@5 for query q3 using sentence-transformers/all-MiniLM-L6-v2 with method tab_embedding is 0.04669349224141275.\n",
      "NDCG@5 for query q4 using sentence-transformers/all-MiniLM-L6-v2 with method tab_embedding is 1.0337230156568273.\n",
      "NDCG@5 for query q5 using sentence-transformers/all-MiniLM-L6-v2 with method tab_embedding is 0.13686884194453566.\n",
      "NDCG@5 for query q6 using sentence-transformers/all-MiniLM-L6-v2 with method tab_embedding is 0.11704008996913323.\n",
      "NDCG@5 for query q1 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_embedding is 0.10337416629989576.\n",
      "NDCG@5 for query q2 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_embedding is 0.050805629137633625.\n",
      "NDCG@5 for query q3 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_embedding is 0.08872226536470461.\n",
      "NDCG@5 for query q4 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_embedding is 0.9407796284693752.\n",
      "NDCG@5 for query q5 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_embedding is 0.1166801639378775.\n",
      "NDCG@5 for query q6 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_embedding is 0.11505387837692371.\n",
      "NDCG@5 for query q1 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_ref_embedding is 0.12014026969220974.\n",
      "NDCG@5 for query q2 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_ref_embedding is 0.14936185337046368.\n",
      "NDCG@5 for query q3 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_ref_embedding is 0.06469011361246282.\n",
      "NDCG@5 for query q4 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_ref_embedding is 1.061545930096415.\n",
      "NDCG@5 for query q5 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_ref_embedding is 0.10193308754243643.\n",
      "NDCG@5 for query q6 using sentence-transformers/all-MiniLM-L6-v2 with method tab_cap_ref_embedding is 0.14139341635134997.\n",
      "NDCG@5 for query q1 using sentence-transformers/all-MiniLM-L6-v2 with method weighted_embedding is 0.0777460592390758.\n",
      "NDCG@5 for query q2 using sentence-transformers/all-MiniLM-L6-v2 with method weighted_embedding is 0.15346553940294536.\n",
      "NDCG@5 for query q3 using sentence-transformers/all-MiniLM-L6-v2 with method weighted_embedding is 0.042326069275201515.\n",
      "NDCG@5 for query q4 using sentence-transformers/all-MiniLM-L6-v2 with method weighted_embedding is 1.021550492647901.\n",
      "NDCG@5 for query q5 using sentence-transformers/all-MiniLM-L6-v2 with method weighted_embedding is 0.1521959010495332.\n",
      "NDCG@5 for query q6 using sentence-transformers/all-MiniLM-L6-v2 with method weighted_embedding is 0.15459724267708627.\n",
      "NDCG@5 for query q1 using deepset/sentence_bert with method tab_embedding is 0.15184115761645314.\n",
      "NDCG@5 for query q2 using deepset/sentence_bert with method tab_embedding is 0.07045992939284422.\n",
      "NDCG@5 for query q3 using deepset/sentence_bert with method tab_embedding is 0.07364331464172705.\n",
      "NDCG@5 for query q4 using deepset/sentence_bert with method tab_embedding is 1.0.\n",
      "NDCG@5 for query q5 using deepset/sentence_bert with method tab_embedding is 0.13809840774056625.\n",
      "NDCG@5 for query q6 using deepset/sentence_bert with method tab_embedding is 0.1348865312918258.\n",
      "NDCG@5 for query q1 using deepset/sentence_bert with method tab_cap_embedding is 0.1664223474114244.\n",
      "NDCG@5 for query q2 using deepset/sentence_bert with method tab_cap_embedding is 0.049220170887004325.\n",
      "NDCG@5 for query q3 using deepset/sentence_bert with method tab_cap_embedding is 0.04751678377684974.\n",
      "NDCG@5 for query q4 using deepset/sentence_bert with method tab_cap_embedding is 0.9169165661407997.\n",
      "NDCG@5 for query q5 using deepset/sentence_bert with method tab_cap_embedding is 0.03044600124221507.\n",
      "NDCG@5 for query q6 using deepset/sentence_bert with method tab_cap_embedding is 0.03142671429132163.\n",
      "NDCG@5 for query q1 using deepset/sentence_bert with method tab_cap_ref_embedding is 0.23649220787510283.\n",
      "NDCG@5 for query q2 using deepset/sentence_bert with method tab_cap_ref_embedding is 0.05264919078114568.\n",
      "NDCG@5 for query q3 using deepset/sentence_bert with method tab_cap_ref_embedding is 0.08115326611014781.\n",
      "NDCG@5 for query q4 using deepset/sentence_bert with method tab_cap_ref_embedding is 1.0.\n",
      "NDCG@5 for query q5 using deepset/sentence_bert with method tab_cap_ref_embedding is 0.06898124504307686.\n",
      "NDCG@5 for query q6 using deepset/sentence_bert with method tab_cap_ref_embedding is 0.0643785567906812.\n",
      "NDCG@5 for query q1 using deepset/sentence_bert with method weighted_embedding is 0.15168731162177831.\n",
      "NDCG@5 for query q2 using deepset/sentence_bert with method weighted_embedding is 0.09106813764198134.\n",
      "NDCG@5 for query q3 using deepset/sentence_bert with method weighted_embedding is 0.03917931481685974.\n",
      "NDCG@5 for query q4 using deepset/sentence_bert with method weighted_embedding is 1.0.\n",
      "NDCG@5 for query q5 using deepset/sentence_bert with method weighted_embedding is 0.0666154321193314.\n",
      "NDCG@5 for query q6 using deepset/sentence_bert with method weighted_embedding is 0.0592831517838513.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "K = 5\n",
    "idcg_values: dict[str, float] = {}\n",
    "\n",
    "for query_id, ranking in ground_truth.items():    \n",
    "    idcg = 0\n",
    "\n",
    "    for i in range(1, K + 1):\n",
    "        table = ground_truth[query_id][str(i)]\n",
    "        table_id = table[\"paper_id\"] + \"#\" + table[\"table_id\"]\n",
    "        rel = float(table[\"rel\"])\n",
    "\n",
    "        idcg += rel / math.log2(i + 1)\n",
    "    \n",
    "    idcg_values[query_id] = idcg\n",
    "\n",
    "for model, methods in results.items():\n",
    "    for method, queries in methods.items():\n",
    "        for query_id, ranking in queries.items():\n",
    "            dcg = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                rel: float = 0\n",
    "                \n",
    "                for _, gt_table in ground_truth[query_id].items():\n",
    "                   gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                   if table_id == gt_table_id:\n",
    "                       rel = float(gt_table[\"rel\"])\n",
    "                \n",
    "                dcg += rel / math.log2(int(pos) + 1)\n",
    "                \n",
    "            ndcg = dcg / idcg_values[query_id]\n",
    "            \n",
    "            print(f\"NDCG@{K} for query {query_id} using {model} with method {method} is {ndcg}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Precision@15 of lucene with bm25 for query q1 is: 0.03261738261738261\n",
      "Avg Precision@15 of lucene with bm25 for query q2 is: 0.08232637732637732\n",
      "Avg Precision@15 of lucene with bm25 for query q3 is: 0.08232637732637732\n",
      "Avg Precision@15 of lucene with bm25 for query q4 is: 0.6121328671328672\n",
      "Avg Precision@15 of lucene with bm25 for query q5 is: 0.10853350353350354\n",
      "Avg Precision@15 of lucene with bm25 for query q6 is: 0.2103901653901654\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q1 is: 0.22121526621526624\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q2 is: 0.2790971990971991\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q3 is: 0.12687497687497687\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q4 is: 0.42573241573241566\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q5 is: 0.0044444444444444444\n",
      "Avg Precision@15 of bert-base-uncased with tab_embedding for query q6 is: 0.0\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q1 is: 0.23687497687497686\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q2 is: 0.24110537610537605\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q3 is: 0.08838291338291339\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q4 is: 0.37848540348540344\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q5 is: 0.03039516039516039\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_embedding for query q6 is: 0.1306845006845007\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q1 is: 0.3757638657638659\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q2 is: 0.3424305324305325\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q3 is: 0.3083812483812484\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q4 is: 0.30219077219077217\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q5 is: 0.0\n",
      "Avg Precision@15 of bert-base-uncased with tab_cap_ref_embedding for query q6 is: 0.02595071595071595\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q1 is: 0.3424305324305325\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q2 is: 0.30354164354164365\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q3 is: 0.1306845006845007\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q4 is: 0.43244958744958745\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q5 is: 0.014334554334554335\n",
      "Avg Precision@15 of bert-base-uncased with weighted_embedding for query q6 is: 0.04002479002479002\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q1 is: 0.19457338957338954\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q2 is: 0.16375494875494878\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q3 is: 0.07264217264217264\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q4 is: 0.5971080771080771\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q5 is: 0.09899304399304398\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_embedding for query q6 is: 0.08232637732637732\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q1 is: 0.2202083102083102\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q2 is: 0.09899304399304398\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q3 is: 0.0525074925074925\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q4 is: 0.4897816997816998\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q5 is: 0.019890109890109888\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_embedding for query q6 is: 0.18049931549931547\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q1 is: 0.3757638657638659\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q2 is: 0.3424305324305325\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q3 is: 0.32412198912198914\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q4 is: 0.03354090354090354\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q5 is: 0.0\n",
      "Avg Precision@15 of distilbert-base-uncased with tab_cap_ref_embedding for query q6 is: 0.0\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q1 is: 0.25354164354164355\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q2 is: 0.18716598216598213\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q3 is: 0.07518185518185519\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q4 is: 0.3979803529803529\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q5 is: 0.014334554334554335\n",
      "Avg Precision@15 of distilbert-base-uncased with weighted_embedding for query q6 is: 0.068993043993044\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q1 is: 0.1212152662152662\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q2 is: 0.22121526621526624\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q3 is: 0.08232637732637732\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q4 is: 0.41020498020498003\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q5 is: 0.04835812335812337\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_embedding for query q6 is: 0.04002479002479002\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q1 is: 0.2612400562400562\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q2 is: 0.15454859954859962\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q3 is: 0.0\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q4 is: 0.09282735782735782\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q5 is: 0.019890109890109888\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_embedding for query q6 is: 0.0\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q1 is: 0.25354164354164355\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q2 is: 0.22354164354164352\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q3 is: 0.15496854996854997\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q4 is: 0.013650793650793651\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q5 is: 0.0\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with tab_cap_ref_embedding for query q6 is: 0.0\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q1 is: 0.23687497687497686\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q2 is: 0.16375494875494878\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q3 is: 0.07290265290265291\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q4 is: 0.1816368816368816\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q5 is: 0.04002479002479002\n",
      "Avg Precision@15 of allenai/scibert_scivocab_uncased with weighted_embedding for query q6 is: 0.0\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q1 is: 0.0\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q2 is: 0.04835812335812337\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q3 is: 0.02354090354090354\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q4 is: 0.7086937136937137\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q5 is: 0.08142283642283642\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_embedding for query q6 is: 0.10692622192622192\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q1 is: 0.068993043993044\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q2 is: 0.04835812335812337\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q3 is: 0.08383264883264882\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q4 is: 0.7066566766566766\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q5 is: 0.18282569282569286\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_embedding for query q6 is: 0.35734543234543226\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q1 is: 0.21243053243053242\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q2 is: 0.16957338957338955\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q3 is: 0.1513194213194213\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q4 is: 0.7358630258630259\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q5 is: 0.12235116735116737\n",
      "Avg Precision@15 of all-mpnet-base-v2 with tab_cap_ref_embedding for query q6 is: 0.2604935804935805\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q1 is: 0.06232637732637733\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q2 is: 0.09899304399304398\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q3 is: 0.019890109890109888\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q4 is: 0.7820213120213119\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q5 is: 0.1016104266104266\n",
      "Avg Precision@15 of all-mpnet-base-v2 with weighted_embedding for query q6 is: 0.1839079439079439\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q1 is: 0.0044444444444444444\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q2 is: 0.19457338957338954\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q3 is: 0.2202083102083102\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q5 is: 0.0044444444444444444\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_embedding for query q6 is: 0.03422466422466422\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q1 is: 0.0\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q2 is: 0.0\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q3 is: 0.08232637732637732\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q4 is: 0.5570832870832871\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q5 is: 0.08827709327709327\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_embedding for query q6 is: 0.16613016613016612\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q1 is: 0.0\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q2 is: 0.16124005624005625\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q3 is: 0.03422466422466422\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q5 is: 0.02595071595071595\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with tab_cap_ref_embedding for query q6 is: 0.10827302327302328\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q1 is: 0.0\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q2 is: 0.09494375994375993\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q3 is: 0.1887732637732638\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q4 is: 0.6538541088541089\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q5 is: 0.08345580345580346\n",
      "Avg Precision@15 of sentence-transformers/sentence-t5-large with weighted_embedding for query q6 is: 0.11007196507196507\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q1 is: 0.068993043993044\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q2 is: 0.10343748843748843\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q3 is: 0.07264217264217264\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q4 is: 0.7042492692492692\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q5 is: 0.10853350353350354\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_embedding for query q6 is: 0.04787545787545787\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q1 is: 0.1513194213194213\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q2 is: 0.02595071595071595\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q3 is: 0.24293151293151294\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q4 is: 0.6538541088541089\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q5 is: 0.18856643356643357\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_embedding for query q6 is: 0.18023310023310027\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q1 is: 0.17909719909719907\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q2 is: 0.1306845006845007\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q3 is: 0.1813194213194213\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q4 is: 0.7086937136937137\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q5 is: 0.11544640544640546\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with tab_cap_ref_embedding for query q6 is: 0.17967754467754468\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q1 is: 0.05991489991489991\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q2 is: 0.13554982054982054\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q3 is: 0.03515706515706516\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q4 is: 0.7183233433233434\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q5 is: 0.18678987678987677\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L6-v2 with weighted_embedding for query q6 is: 0.19160469160469157\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_embedding for query q1 is: 0.24110537610537605\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_embedding for query q2 is: 0.08232637732637732\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_embedding for query q3 is: 0.2202083102083102\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_embedding for query q5 is: 0.1034052984052984\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_embedding for query q6 is: 0.09279276279276279\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_embedding for query q1 is: 0.2902083102083103\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_embedding for query q2 is: 0.019890109890109888\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_embedding for query q3 is: 0.10819939319939319\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_embedding for query q4 is: 0.5959721759721762\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_embedding for query q5 is: 0.019890109890109888\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_embedding for query q6 is: 0.02595071595071595\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_ref_embedding for query q1 is: 0.3757638657638659\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_ref_embedding for query q2 is: 0.03261738261738261\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_ref_embedding for query q3 is: 0.1839079439079439\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_ref_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_ref_embedding for query q5 is: 0.03039516039516039\n",
      "Avg Precision@15 of deepset/sentence_bert with tab_cap_ref_embedding for query q6 is: 0.05756447256447257\n",
      "Avg Precision@15 of deepset/sentence_bert with weighted_embedding for query q1 is: 0.27576386576386575\n",
      "Avg Precision@15 of deepset/sentence_bert with weighted_embedding for query q2 is: 0.1212152662152662\n",
      "Avg Precision@15 of deepset/sentence_bert with weighted_embedding for query q3 is: 0.04584082584082584\n",
      "Avg Precision@15 of deepset/sentence_bert with weighted_embedding for query q4 is: 0.6782985532985534\n",
      "Avg Precision@15 of deepset/sentence_bert with weighted_embedding for query q5 is: 0.013650793650793651\n",
      "Avg Precision@15 of deepset/sentence_bert with weighted_embedding for query q6 is: 0.029096459096459096\n"
     ]
    }
   ],
   "source": [
    "# Precision at k\n",
    "def precision_at_k(k: int):\n",
    "    precision_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "    for model, methods in results.items():\n",
    "        precision_values[model] = {}\n",
    "        for method, queries in methods.items():\n",
    "            precision_values[model][method] = {}\n",
    "            for query_id, ranking in queries.items():\n",
    "                relevant = 0\n",
    "                for pos, table_id in ranking.items():\n",
    "                    if int(pos) > k: break\n",
    "                    \n",
    "                    rel: float = 0\n",
    "                    \n",
    "                    # find relevance\n",
    "                    for _, gt_table in ground_truth[query_id].items():\n",
    "                        gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                        if table_id == gt_table_id:\n",
    "                            rel = float(gt_table[\"rel\"])\n",
    "                    \n",
    "                    if rel > 0: relevant += 1\n",
    "                \n",
    "                precision = relevant / k\n",
    "                precision_values[model][method][query_id] = precision\n",
    "    \n",
    "    return precision_values\n",
    "\n",
    "# Avg Precision at K\n",
    "K = 15\n",
    "# model -> method -> query, avg_precision@k\n",
    "ap_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    ap_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        ap_values[model][method] = {}\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            sum_p = 0\n",
    "            \n",
    "            for k in range(1, K + 1):\n",
    "                precision_values_at_k = precision_at_k(k)\n",
    "                sum_p += precision_values_at_k[model][method][query_id]\n",
    "                \n",
    "            ap_values[model][method][query_id] = sum_p / K\n",
    "            print(f\"Avg Precision@{K} of {model} with {method} for query {query_id} is: {ap_values[model][method][query_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@15 for lucene using method: bm25 is 0.1880544455544456.\n",
      "MAP@15 for bert-base-uncased using method: tab_embedding is 0.1762273837273837.\n",
      "MAP@15 for bert-base-uncased using method: tab_cap_embedding is 0.18432138848805515.\n",
      "MAP@15 for bert-base-uncased using method: tab_cap_ref_embedding is 0.2257861891195225.\n",
      "MAP@15 for bert-base-uncased using method: weighted_embedding is 0.21057760141093476.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_embedding is 0.20156633489966824.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_cap_embedding is 0.17697999531332864.\n",
      "MAP@15 for distilbert-base-uncased using method: tab_cap_ref_embedding is 0.17930954847621516.\n",
      "MAP@15 for distilbert-base-uncased using method: weighted_embedding is 0.16619957203290533.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_embedding is 0.1538908005574672.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_cap_embedding is 0.08808435391768725.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: tab_cap_ref_embedding is 0.10761710511710514.\n",
      "MAP@15 for allenai/scibert_scivocab_uncased using method: weighted_embedding is 0.11586570836570835.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_embedding is 0.16149029982363317.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_cap_embedding is 0.241335269668603.\n",
      "MAP@15 for all-mpnet-base-v2 using method: tab_cap_ref_embedding is 0.27533851950518623.\n",
      "MAP@15 for all-mpnet-base-v2 using method: weighted_embedding is 0.20812486895820226.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_embedding is 0.18936563436563436.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_cap_embedding is 0.14896948730282067.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: tab_cap_ref_embedding is 0.16799783549783553.\n",
      "MAP@15 for sentence-transformers/sentence-t5-large using method: weighted_embedding is 0.18851648351648354.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_embedding is 0.1842884892884893.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_embedding is 0.24047588214254886.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: tab_cap_ref_embedding is 0.2491531308197975.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L6-v2 using method: weighted_embedding is 0.22122328288994955.\n",
      "MAP@15 for deepset/sentence_bert using method: tab_embedding is 0.23635611302277967.\n",
      "MAP@15 for deepset/sentence_bert using method: tab_cap_embedding is 0.17668513585180257.\n",
      "MAP@15 for deepset/sentence_bert using method: tab_cap_ref_embedding is 0.2264245630912298.\n",
      "MAP@15 for deepset/sentence_bert using method: weighted_embedding is 0.19397762731096066.\n"
     ]
    }
   ],
   "source": [
    "# MAP@K (K è quello sopra)\n",
    "map_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    map_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_ap = 0\n",
    "        \n",
    "        for avg_prec in ap_values[model][method].values():\n",
    "            sum_ap += avg_prec\n",
    "        \n",
    "        map_value = sum_ap / num_queries\n",
    "        map_values[model][method] = map_value\n",
    "            \n",
    "        print(f\"MAP@{K} for {model} using method: {method} is {map_value}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
