{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione\n",
    "\n",
    "1. Creare tot sistemi diversi (per esempio, lucene, bert, scibert, tabert (con e senza contesto))\n",
    "2. Creare un sottoinsieme dei paper (tipo 20) da usare come ground truth (o a caso oppure con lucene i più rilevanti per argomento che abbiano tabelle interessanti) - cerchiamo di limitare il numero di tabelle a ca. 50\n",
    "3. Per ogni query $q \\in Q$ (min 5):\n",
    "    1. Fare il ranking a mano delle tabelle\n",
    "        - Salviamo i ranking per ogni query in un json, con le informazioni rilevanti, tipo il ranking, il valore di rilevanza per ogni elemento etc.\n",
    "    2. Interrogare ogni sistema sulla query\n",
    "    3. Calcolare le metriche: \n",
    "        - Reciprocal Rank: $\\text{RR}_q = \\frac{1}{rank_i}$ dove $i$ è l’elemento più rilevante.\n",
    "            - nella pratica possiamo controllare se l’elemento scelto dal motore ha almeno lo score massimo (potrebbero esserci dei parimerito)\n",
    "        - Normalized Discounted Cumulative Gain con taglio $\\text{K} = \\set{5,15}$:\n",
    "            \n",
    "            $$\n",
    "            \\text{NDCG@K}_q = \\frac{\\text{DCG@K}_q}{\\text{IDCG@K}_q}\n",
    "            $$\n",
    "            \n",
    "            - dove dividiamo il $\\text{DCG@K}_q = rel_1 + \\sum_{i=2}^K \\frac{rel_i}{\\log_2 (i + 1)}$ con quello ideale, cioè dove il ranking è il migliore possibile\n",
    "4. Calcolare la media delle metriche:\n",
    "    - Mean Reciprocal Rank: $\\text{MRR} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{RR}_q$\n",
    "    - Media dei NDCG: $\\frac{1}{|Q|} \\sum_{q \\in Q} \\text{NDCG@K}_q$\n",
    "\n",
    "### Query (in verde stesso ranking ma proviamo sinonimi)\n",
    "\n",
    "1. NDCG su dataset movielens ✅\n",
    "2. Recommender systems Recall su dataset goodbook ✅\n",
    "3. Recommender systems MRR ✅\n",
    "4. Deep Learning dataset Apple Flower ✅\n",
    "5. Deep Learning GPT3 precision f1 ✅\n",
    "6. Deep Learning GPT3 precision f-measure ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "results_file = \"./results_hybrid.json\"\n",
    "ground_truth_path = \"./ground_truth\"\n",
    "num_queries = 6\n",
    "\n",
    "# model -> method -> query -> (position, id) \n",
    "results: dict[str, dict[str, dict[str, dict[str, str]]]] = {}\n",
    "\n",
    "# query -> (position, table[table_id, query_id, rel]) \n",
    "ground_truth: dict[str, dict[str, dict[str, str]]] = {}\n",
    "\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "    \n",
    "for i in range(1, num_queries + 1):\n",
    "    query_id = f\"q{i}\"\n",
    "    with open(ground_truth_path + f\"/{query_id}_rank.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        ground_truth[query_id] = json.load(file)\n",
    "            \n",
    "\n",
    "def compare_id(id1: str, id2: str) -> bool:\n",
    "    id1 = str.lower(re.sub(r'v\\d+', '', id1))\n",
    "    id2 = str.lower(re.sub(r'v\\d+', '', id2))\n",
    "\n",
    "    return id1 == id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR value for lucene using method: bm25 is 0.11666666666666665 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_embedding is 0.044444444444444446 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_embedding is 0.0625 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_ref_embedding is 0.09722222222222221 --- best not found in 4/6 queries.\n",
      "MRR value for sentence-transformers/all-MiniLM-L12-v2 using method: weighted_embedding is 0.06547619047619048 --- best not found in 4/6 queries.\n"
     ]
    }
   ],
   "source": [
    "mrr_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    mrr_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_rr = 0\n",
    "        not_founds = 0\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            best_tables_ids: list[str] = []\n",
    "            best_table =  ground_truth[query_id][\"1\"]\n",
    "            \n",
    "            # this checks for equal relevance tables other than the first position\n",
    "            for pos, table in ground_truth[query_id].items():\n",
    "                if table[\"rel\"] == best_table[\"rel\"]:\n",
    "                    best_tables_ids.append(table[\"paper_id\"] + \"#\" + table[\"table_id\"])\n",
    "            \n",
    "            rr = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                if (table_id in best_tables_ids): rr = 1.0 / float(pos)\n",
    "            \n",
    "            if rr == 0: not_founds += 1\n",
    "            sum_rr += rr\n",
    "            \n",
    "        mrr = sum_rr / num_queries\n",
    "        mrr_values[model][method] = mrr\n",
    "        print(f\"MRR value for {model} using method: {method} is {mrr} --- best not found in {not_founds}/{num_queries} queries.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== lucene --> bm25 ==============\n",
      "NDCG@15 for query q1 is 0.6869004377806107.\n",
      "NDCG@15 for query q2 is 0.6155781827914666.\n",
      "NDCG@15 for query q3 is 0.6518457539071132.\n",
      "NDCG@15 for query q4 is 0.7449879246332356.\n",
      "NDCG@15 for query q5 is 0.8128905183855183.\n",
      "NDCG@15 for query q6 is 0.7202068137152581.\n",
      "Average NDCG@15 is 0.7054016052022005.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_embedding ==============\n",
      "NDCG@15 for query q1 is 0.7528995341218093.\n",
      "NDCG@15 for query q2 is 0.7326052251146056.\n",
      "NDCG@15 for query q3 is 0.8623837149504089.\n",
      "NDCG@15 for query q4 is 0.7452132005667871.\n",
      "NDCG@15 for query q5 is 0.7989492277064728.\n",
      "NDCG@15 for query q6 is 0.7944230678634309.\n",
      "Average NDCG@15 is 0.7810789950539191.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_cap_embedding ==============\n",
      "NDCG@15 for query q1 is 0.774200281160445.\n",
      "NDCG@15 for query q2 is 0.4744732352515133.\n",
      "NDCG@15 for query q3 is 0.6271579350957118.\n",
      "NDCG@15 for query q4 is 0.7275542565373209.\n",
      "NDCG@15 for query q5 is 0.443841726457942.\n",
      "NDCG@15 for query q6 is 0.47601407709405613.\n",
      "Average NDCG@15 is 0.5872069185994983.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> tab_cap_ref_embedding ==============\n",
      "NDCG@15 for query q1 is 0.7792675104116694.\n",
      "NDCG@15 for query q2 is 0.7970525814164617.\n",
      "NDCG@15 for query q3 is 0.8057638136362262.\n",
      "NDCG@15 for query q4 is 0.7678016385941904.\n",
      "NDCG@15 for query q5 is 0.8205053505967671.\n",
      "NDCG@15 for query q6 is 0.814765450263981.\n",
      "Average NDCG@15 is 0.7975260574865493.\n",
      "\n",
      "\n",
      "============== sentence-transformers/all-MiniLM-L12-v2 --> weighted_embedding ==============\n",
      "NDCG@15 for query q1 is 0.7884755713977798.\n",
      "NDCG@15 for query q2 is 0.7755703809558128.\n",
      "NDCG@15 for query q3 is 0.6696614164915076.\n",
      "NDCG@15 for query q4 is 0.7902924053339736.\n",
      "NDCG@15 for query q5 is 0.7341023996456951.\n",
      "NDCG@15 for query q6 is 0.7358773982156733.\n",
      "Average NDCG@15 is 0.7489965953400737.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "K = 15\n",
    "idcg_values: dict[str, float] = {}\n",
    "\n",
    "for query_id, ranking in ground_truth.items():    \n",
    "    idcg = 0\n",
    "\n",
    "    for i in range(1, K + 1):\n",
    "        table = ground_truth[query_id][str(i)]\n",
    "        table_id = table[\"paper_id\"] + \"#\" + table[\"table_id\"]\n",
    "        rel: float = float(table[\"rel\"])\n",
    "\n",
    "        idcg += rel / math.log2(i + 1)\n",
    "    \n",
    "    idcg_values[query_id] = idcg\n",
    "\n",
    "for model, methods in results.items():\n",
    "    for method, queries in methods.items():\n",
    "        print(f\"============== {model} --> {method} ==============\")\n",
    "        sum_ndcg: float = 0\n",
    "        for query_id, ranking in queries.items():\n",
    "            dcg = 0\n",
    "            for pos, table_id in ranking.items():\n",
    "                rel: float = 0\n",
    "                \n",
    "                for _, gt_table in ground_truth[query_id].items():\n",
    "                   gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                   if compare_id(table_id, gt_table_id):\n",
    "                       rel = float(gt_table[\"rel\"])\n",
    "                \n",
    "                dcg += rel / math.log2(int(pos) + 1)\n",
    "                \n",
    "            ndcg = dcg / idcg_values[query_id] if dcg / idcg_values[query_id] <= 1 else 1\n",
    "            sum_ndcg += ndcg\n",
    "            \n",
    "            print(f\"NDCG@{K} for query {query_id} is {ndcg}.\")\n",
    "        print(f\"Average NDCG@{K} is {sum_ndcg / num_queries}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Precision@15 of lucene with bm25 for query q1 is: 0.8452438302438303\n",
      "Avg Precision@15 of lucene with bm25 for query q2 is: 0.7926321826321825\n",
      "Avg Precision@15 of lucene with bm25 for query q3 is: 0.7241393791393791\n",
      "Avg Precision@15 of lucene with bm25 for query q4 is: 0.8088226588226587\n",
      "Avg Precision@15 of lucene with bm25 for query q5 is: 0.8191584341584339\n",
      "Avg Precision@15 of lucene with bm25 for query q6 is: 0.8052190402190401\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q1 is: 0.6972272172272173\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q2 is: 0.8200843600843599\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q3 is: 0.895849890849891\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q4 is: 0.7109159359159359\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q5 is: 0.82506364006364\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_embedding for query q6 is: 0.831124246124246\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q1 is: 0.858894623894624\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q2 is: 0.44866152366152373\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q3 is: 0.7763301513301513\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q4 is: 0.6295130795130796\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q5 is: 0.5824190624190624\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_embedding for query q6 is: 0.602924112924113\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q1 is: 0.7431002331002331\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q2 is: 0.8482043882043884\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q3 is: 0.8801091501091503\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q4 is: 0.7266566766566768\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q5 is: 0.8650884300884301\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with tab_cap_ref_embedding for query q6 is: 0.8448808598808597\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q1 is: 0.8202166352166353\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q2 is: 0.8516981166981168\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q3 is: 0.6634193584193583\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q4 is: 0.7621312021312021\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q5 is: 0.7423123173123173\n",
      "Avg Precision@15 of sentence-transformers/all-MiniLM-L12-v2 with weighted_embedding for query q6 is: 0.7049330299330298\n"
     ]
    }
   ],
   "source": [
    "# Precision at k\n",
    "def precision_at_k(k: int):\n",
    "    precision_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "    for model, methods in results.items():\n",
    "        precision_values[model] = {}\n",
    "        for method, queries in methods.items():\n",
    "            precision_values[model][method] = {}\n",
    "            for query_id, ranking in queries.items():\n",
    "                relevant = 0\n",
    "                for pos, table_id in ranking.items():\n",
    "                    if int(pos) > k: break\n",
    "                    \n",
    "                    rel: float = 0\n",
    "                    \n",
    "                    # find relevance\n",
    "                    for _, gt_table in ground_truth[query_id].items():\n",
    "                        gt_table_id = gt_table[\"paper_id\"] + \"#\" + gt_table[\"table_id\"]\n",
    "                        if compare_id(table_id, gt_table_id):\n",
    "                            rel = float(gt_table[\"rel\"])\n",
    "                    \n",
    "                    if rel > 0: relevant += 1\n",
    "                \n",
    "                precision = relevant / k\n",
    "                precision_values[model][method][query_id] = precision\n",
    "    \n",
    "    return precision_values\n",
    "\n",
    "# Avg Precision at K\n",
    "K = 15\n",
    "# model -> method -> query, avg_precision@k\n",
    "ap_values: dict[str, dict[str, dict[str, float]]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    ap_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        ap_values[model][method] = {}\n",
    "        \n",
    "        for query_id, ranking in queries.items():\n",
    "            sum_p = 0\n",
    "            \n",
    "            for k in range(1, K + 1):\n",
    "                precision_values_at_k = precision_at_k(k)\n",
    "                sum_p += precision_values_at_k[model][method][query_id]\n",
    "                \n",
    "            ap_values[model][method][query_id] = sum_p / K\n",
    "            print(f\"Avg Precision@{K} of {model} with {method} for query {query_id} is: {ap_values[model][method][query_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@15 for lucene using method: bm25 is 0.7992025875359209.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_embedding is 0.7967108817108817.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_embedding is 0.649790425623759.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: tab_cap_ref_embedding is 0.8180066230066231.\n",
      "MAP@15 for sentence-transformers/all-MiniLM-L12-v2 using method: weighted_embedding is 0.7574517766184433.\n"
     ]
    }
   ],
   "source": [
    "# MAP@K (K è quello sopra)\n",
    "map_values: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for model, methods in results.items():\n",
    "    map_values[model] = {}\n",
    "    for method, queries in methods.items():\n",
    "        sum_ap = 0\n",
    "        \n",
    "        for avg_prec in ap_values[model][method].values():\n",
    "            sum_ap += avg_prec\n",
    "        \n",
    "        map_value = sum_ap / num_queries\n",
    "        map_values[model][method] = map_value\n",
    "            \n",
    "        print(f\"MAP@{K} for {model} using method: {method} is {map_value}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
