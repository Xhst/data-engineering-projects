{
  "Domain Adaptation Methods": [
    "Virtual KITTI→KITTI",
    "Cityscapes",
    "Cross-City",
    "Tsai etal.",
    "Sankaranarayanan etal.",
    "CBST",
    "FCNs Wld",
    "image translation with CycleGAN",
    "ROAD-Net",
    "FinetuneGAN",
    "FinetuneGAN, Flip"
  ],

  "Street Scenes - Environment": [
    "road",
    "sidewalk",
    "fence",
    "pole",
    "traffic light",
    "traffic sign",
    "building",
    "wall",
    "vegetation",
    "terrain",
    "sky"
  ],

  "Street Scenes - People": [
    "person",
    "rider"
  ],

  "Street Scenes - Two-Wheel Vehicles": [
    "motorbike",
    "bicycle"
  ],

  "Street Scenes - Four-Wheel Vehicles": [
    "car",
    "truck",
    "Truck",
    "bus"
  ],

  "Real Data": [
    "Real data",
    "Real-only",
    "Real only",
    "Videos+Real",
    "Dataset"
  ],

  "Synthetic Data": [
    "Synthetic data",
    "Synthetic Data",
    "11.6K synthetic test instances",
    "Syn-only",
    "CP-only",
    "CP-Hybrid"
  ],

  "Image Classification Datasets": [
    "CUB",
    "NAB",
    "ImageNet",
    "CIFAR-10"
  ],

  "Video Action Recognition Datasets": [
    "HMDB-51",
    "HMDB-38",
    "UCF-101",
    "UCF-25"
  ],

  "Few-Shot / Meta-Learning": [
    "ProtoNet",
    "MatchingNet",
    "MAML",
    "RelationNet",
    "Conv4"
  ],

  "Basic Classification Baselines": [
    "Nearest Neighbor",
    "Nearest neighbor",
    "Logistic Regression",
    "Logistic regression",
    "Random"
  ],

  "Video-based Approaches": [
    "CoViar",
    "OFF",
    "iDT+FV",
    "IFD",
    "SEDB & IFD",
    "SEDB"
  ],

  "Architectures - MetaIRNet/ResNet Family": [
    "MetaIRNet (Ours)",
    "MetaIRNet",
    "ResNet18",
    "3D-ResNet-101",
    "ResNet100",
    "ResNet50",
    "MobileFaceNet"
  ],

  "NN / CNN References": [
    "NN architecture",
    "CNN"
  ],

  "Federated Learning": [
    "Cross-Silo Horizontal FL",
    "FL architecture",
    "FedAvg",
    "FedProx",
    "Number of clients"
  ],

  "Federated Learning Hyperparameters": [
    "Number of local epochs",
    "Number of global rounds"
  ],

  "Optimizers and Schedulers": [
    "Optimizer",
    "Optimizer β 1",
    "Optimizer β 2",
    "Optimizer ϵ",
    "Adamw",
    "Stochastic gradient descent",
    "Learning Rate",
    "Learning rate",
    "Learning Rate Scheduler",
    "Curriculum",
    "Training",
    "Cosine",
    "Warmup Step",
    "Epochs",
    "Seed"
  ],

  "Precision Settings": [
    "AMP",
    "FP32",
    "bfloat16",
    "w8a8",
    "w6a6"
  ],

  "Prompting Strategies": [
    "2-Shot Prompting",
    "8-Shot Prompting",
    "Chain-of-Thought"
  ],

  "Fruits": [
    "Strawberry",
    "Peach",
    "Plum",
    "Apple",
    "Banana",
    "Orange"
  ],

  "CIFAR-10 Classes": [
    "Plane",
    "Cat",
    "Ship"
  ],

  "Llama Family": [
    "Llama-2-7B",
    "Llama-3-8B-Instruct",
    "Llama-3-70B-Instruct"
  ],

  "Qwen Family": [
    "Yi-1.5-34B-Chat",
    "Qwen1.5-72B-Chat",
    "Qwen1.5-110B-Chat",
    "Qwen2-7B-Instruct",
    "Qwen2-72B-Instruct",
    "Qwen2-7B (fine-tuned w/ the 1.07M synthesized instances)"
  ],

  "DeepSeek Family": [
    "Deepseek-Coder-33B",
    "DeepSeek LLM 67B Chat",
    "DeepSeek-Coder-V2-Instruct"
  ],

  "GPT-4 Turbo Family": [
    "gpt-4-turbo-2024-04-09",
    "gpt-4-turbo-0125-preview",
    "gpt-4-turbo-1106-preview"
  ],

  "GPT-4": [
    "gpt-4o-2024-05-13",
    "gpt-4",
    "GPT4"
  ],

  "Claude Family": [
    "Claude 3.5 Sonnet",
    "Claude 3 Opus"
  ],

  "Other LLM References": [
    "Phi-3-Mini-4K-Instruct",
    "Gemini Pro 1.5 (May 2024)",
    "Gemini Ultra",
    "236B/21B"
  ],

  "Adaptation Levels": [
    "Non Adapt",
    "Input Level Adapt",
    "Output Level Adapt",
    "Input&Ouput Adapt",
    "Ours (Non Adapt)",
    "Ours (Input-level Adapt)",
    "Ours (Output-level Adapt)",
    "Ours (Input&Output Adapt)"
  ],

  "Stages": [
    "Stage 3+",
    "Stage 4+",
    "Stage 5+"
  ],

  "DCASE": [
    "DCASE 2018",
    "DCASE 2019",
    "DPSDA-FL"
  ],

  "Non-Adaptive Baseline vs. Softmax": [
    "non-adaptive baseline",
    "Softmax Regression",
    "Softmax regression",
    "baseline"
  ],

  "Output Alignment Approaches": [
    "semantic segmentation output map",
    "depth output map",
    "individually aligning both semantic segmentation and depth estimation",
    "joint output space of depth estimation and semantic segmentation"
  ],

  "Image Transform Network Variants": [
    "image transform network guided by task network",
    "image transform network guided by task network with additional depth input",
    "image transform network guided by task network with additional semantic label input",
    "image transform network guided by task network with both depth and semantic labels as additional input"
  ],

  "Data Composition": [
    "Original",
    "Original + Generated",
    "Original + Mixed",
    "Gen-Hybrid"
  ],

  "TSN Variants": [
    "TSN-2M",
    "TSN-3M",
    "Cool-TSN"
  ],

  "Two-stream Variants": [
    "Two-stream",
    "Two-stream LSTM",
    "L 2 STM",
    "Two-stream MiCT",
    "Two-stream I3D"
  ],

  "Network-1 and Network-2": [
    "Network-1",
    "Network-2",
    "Network-2 (half of the generated videos)",
    "Network-2 (all of the generated videos)"
  ],

  "LoRA Settings": [
    "LoRA Rank",
    "LoRA α",
    "LoRA Dropout"
  ],

  "Other": [
    "Heads",
    "Sampling",
    "K-means",
    "DBSCAN",
    "Labelling",
    "Grasping",
    "ID",
    "Example of prompt and response",
    "background",
    "simplified"
  ]
}
